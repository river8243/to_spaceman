{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import logging\n",
    "from esun_phoneme_tool.tokenizer import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import math\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import tokenize_and_map, RunningAverage\n",
    "\n",
    "CN_EN_RE = r'[\\u4e00-\\u9fa5A-Za-z]'\n",
    "FORMAT = '%(asctime)s %(levelname)s: %(message)s'\n",
    "logging.basicConfig(level=logging.DEBUG, format=FORMAT)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, texts, trues=None,\n",
    "                 pad_token_label_id=0, max_length=512, for_train=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.trues = trues\n",
    "        self.pad_token_label_id = pad_token_label_id\n",
    "        self.max_length = max_length\n",
    "        self.for_train = for_train\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        q_texts = self.texts[idx]\n",
    "\n",
    "        processed_tokens = ['[CLS]'] \n",
    "        for text in q_texts:\n",
    "            tokens, text2token, token2text = tokenize_and_map(self.tokenizer, text)\n",
    "\n",
    "            cut_index = self.max_length - 50\n",
    "            if cut_index < len(tokens):\n",
    "                cut_text_index = text2token.index(cut_index)\n",
    "                tokens = tokens[:cut_index]\n",
    "                            \n",
    "            processed_tokens += tokens + ['[SEP]']\n",
    "            \n",
    "        input_ids = torch.tensor(self.tokenizer.convert_tokens_to_ids(processed_tokens))\n",
    "        token_type_ids = torch.tensor([0] * len(processed_tokens))\n",
    "        attention_mask = torch.tensor([1] * len(processed_tokens))\n",
    "\n",
    "        outputs = (input_ids, token_type_ids, attention_mask, )\n",
    "\n",
    "        if self.for_train:\n",
    "            true = self.trues[idx]\n",
    "            label = torch.tensor(true)\n",
    "            outputs += (label, )\n",
    "\n",
    "        info = {\n",
    "            'tokens': tokens,\n",
    "            'text': q_texts, # [ ] 並非實際進BERT的句子(512上限)\n",
    "        }\n",
    "        outputs += (info, )\n",
    "        return outputs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def create_mini_batch(self, samples):\n",
    "        outputs = list(zip(*samples))\n",
    "\n",
    "        # zero pad 到同一序列長度\n",
    "        input_ids = pad_sequence(outputs[0], batch_first=True)\n",
    "        token_type_ids = pad_sequence(outputs[1], batch_first=True)\n",
    "        attention_mask = pad_sequence(outputs[2], batch_first=True)\n",
    "\n",
    "        batch_output = (input_ids, token_type_ids, attention_mask)\n",
    "    \n",
    "        if self.for_train:\n",
    "            labels = torch.stack(outputs[3])\n",
    "            batch_output += (labels, )\n",
    "        else:\n",
    "            infos = outputs[3]\n",
    "            batch_output += (infos, )\n",
    "\n",
    "        return batch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model, data, optimizer, device, criterion):\n",
    "    model.train()\n",
    "    input_ids, token_type_ids, attention_mask, labels = [d.to(device) for d in data]\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        token_type_ids=token_type_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=labels\n",
    "    )\n",
    "\n",
    "    loss = criterion(outputs.logits.view(-1, 2), labels.view(-1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate(model, valid_loader):\n",
    "    model.eval()\n",
    "    device = 'cuda' if next(model.parameters()).is_cuda else 'cpu'\n",
    "\n",
    "    loss_averager = RunningAverage()\n",
    "    acc_averager = RunningAverage()\n",
    "\n",
    "    tp, fp, fn = 0, 0, 0    \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(valid_loader, desc='evaluate'):\n",
    "            input_ids, token_type_ids, attention_mask, labels = [d.to(device) for d in data]\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                token_type_ids=token_type_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss_averager.add(outputs.loss.item())\n",
    "            \n",
    "            corrects = (outputs.logits.argmax(dim=-1) == labels).cpu().tolist()\n",
    "            preds = outputs.logits.argmax(dim=-1).cpu().tolist()\n",
    "\n",
    "            for label, pred in zip(labels, preds):\n",
    "                tp += 1 if (label, pred) == (0, 0) else 0\n",
    "                fp += 1 if (label, pred) == (1, 0) else 0\n",
    "                fn += 1 if (label, pred) == (0, 1) else 0\n",
    "            \n",
    "            acc_averager.add_all(corrects)\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else None\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else None\n",
    "    f1 = 2 / (1 / precision + 1 / recall) if precision and recall else None\n",
    "    \n",
    "    evaluation = {\n",
    "        'loss': loss_averager.get(), \n",
    "        'accuracy':acc_averager.get(),\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def _empty_cache():\n",
    "#     try:\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_main(texts, trues,\n",
    "              model_dir='/home/jovyan/if-beautiful-text/owen_dev/if_beautiful_text/cache_dir/bert-base-chinese',\n",
    "              save_dir='./models/question_check/'):\n",
    "    lr = 0.00001\n",
    "    train_batch_size = 8\n",
    "    evaluate_batch_size = 64\n",
    "    max_iter = 100000\n",
    "    show_train_per_iter = 100\n",
    "    show_eval_per_iter = 100\n",
    "    save_per_iter = 1000\n",
    "    cpu_workers = 4\n",
    "    checkpoint_folder = None\n",
    "\n",
    "    assert save_per_iter % show_eval_per_iter == 0\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logging.info(f'device: {device}')\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "    global SKIP_TOKEN_IDS, SKIP_TOKENS\n",
    "    SKIP_TOKEN_IDS = [tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id]\n",
    "    SKIP_TOKENS = [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]\n",
    "\n",
    "    if not checkpoint_folder:\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            model_dir, \n",
    "            return_dict=True,\n",
    "            num_labels=2)\n",
    "    else:\n",
    "        model = BertForSequenceClassification.from_pretrained(checkpoint_folder)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "#     dataset = TextDataset(tokenizer, texts, trues)\n",
    "\n",
    "#     setup_seed(0)\n",
    "#     CUT_RATIO = 0.8\n",
    "#     train_size = int(CUT_RATIO * len(dataset))\n",
    "#     valid_size = len(dataset) - train_size\n",
    "#     train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "    # t_texts, t_trues = prepare_data('/home/jovyan/wm-insur-call-qa/owen/training_data_generated_train_0119.tsv')\n",
    "    # train_dataset = TextDataset(tokenizer, t_texts, t_trues)\n",
    "\n",
    "    # v_texts, v_trues = prepare_data('/home/jovyan/wm-insur-call-qa/owen/training_data_generated_valid_0119.tsv')\n",
    "    # valid_dataset = TextDataset(tokenizer, v_texts, v_trues)\n",
    "    \n",
    "    CUT_RATIO = 0.8\n",
    "    train_size = int(CUT_RATIO * len(texts))\n",
    "    train_dataset = TextDataset(tokenizer, texts[:train_size], trues[:train_size])\n",
    "    valid_dataset = TextDataset(tokenizer, texts[train_size:], trues[train_size:])\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=train_batch_size,\n",
    "        collate_fn=train_dataset.create_mini_batch,\n",
    "        shuffle=True,\n",
    "        num_workers=cpu_workers)\n",
    "    valid_loader = DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=evaluate_batch_size,\n",
    "        collate_fn=valid_dataset.create_mini_batch,\n",
    "        shuffle=True,\n",
    "        num_workers=cpu_workers)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "    \n",
    "    # customerize weight\n",
    "    weight = [5, 1]\n",
    "    weight = torch.tensor(weight).to(device)\n",
    "    weight = weight.float()\n",
    "    \n",
    "    criterion = CrossEntropyLoss(weight=weight)\n",
    "    \n",
    "    i = 1\n",
    "    is_running = True\n",
    "    loss_averager = RunningAverage()\n",
    "    max_f1 = 0\n",
    "    while is_running:\n",
    "        for train_data in train_loader:\n",
    "            loss = train_batch(model, train_data, optimizer, device, criterion)\n",
    "            loss_averager.add(loss)\n",
    "\n",
    "            if i % show_train_per_iter == 0:\n",
    "                logging.info('train_loss [{iter}]: {train_loss}'.format(\n",
    "                    iter=i, train_loss=loss_averager.get()))\n",
    "                loss_averager.flush()\n",
    "\n",
    "            if i % show_eval_per_iter == 0:\n",
    "                evaluation = evaluate(model, valid_loader)\n",
    "                logging.info('valid_evaluation: loss={loss}, accuracy={accuracy}, '\n",
    "                             'precision={precision}, recall={recall}, f1={f1}'\n",
    "                             .format(**evaluation))\n",
    "                if evaluation['f1']:\n",
    "                    if evaluation['f1'] > max_f1:\n",
    "                        eval_loss = evaluation['loss']\n",
    "                        path = os.path.join(save_dir, f'question_check_weight5_1_step{i}_loss{eval_loss}/')\n",
    "                        logging.info(f'Save model at {path}')\n",
    "                        model.save_pretrained(path)\n",
    "                        max_f1 = evaluation['f1']\n",
    "\n",
    "            if i == max_iter:\n",
    "                is_running = False\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7172"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(8966*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 17:06:41,710 INFO: device: cuda\n",
      "Some weights of the model checkpoint at /home/jovyan/if-beautiful-text/owen_dev/if_beautiful_text/cache_dir/bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/jovyan/if-beautiful-text/owen_dev/if_beautiful_text/cache_dir/bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2021-04-29 17:06:57,270 INFO: train_loss [100]: 0.5521930059790612\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.87it/s]\n",
      "2021-04-29 17:07:07,392 INFO: valid_evaluation: loss=0.30457105410510094, accuracy=0.9007803790412486, precision=0.5576923076923077, recall=0.5742574257425742, f1=0.5658536585365854\n",
      "2021-04-29 17:07:07,393 INFO: Save model at ./models/question_check/question_check_weight5_1_step100_loss0.30457105410510094/\n",
      "2021-04-29 17:07:20,921 INFO: train_loss [200]: 0.3178734134137631\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.85it/s]\n",
      "2021-04-29 17:07:31,092 INFO: valid_evaluation: loss=0.16324050030831633, accuracy=0.9342251950947603, precision=0.6962616822429907, recall=0.7376237623762376, f1=0.7163461538461539\n",
      "2021-04-29 17:07:31,094 INFO: Save model at ./models/question_check/question_check_weight5_1_step200_loss0.16324050030831633/\n",
      "2021-04-29 17:07:44,105 INFO: train_loss [300]: 0.21348304457962514\n",
      "evaluate: 100%|██████████| 29/29 [00:09<00:00,  2.91it/s]\n",
      "2021-04-29 17:07:54,082 INFO: valid_evaluation: loss=0.1575926386847578, accuracy=0.9308807134894092, precision=0.6423357664233577, recall=0.8712871287128713, f1=0.7394957983193278\n",
      "2021-04-29 17:07:54,085 INFO: Save model at ./models/question_check/question_check_weight5_1_step300_loss0.1575926386847578/\n",
      "2021-04-29 17:08:07,627 INFO: train_loss [400]: 0.2129088088683784\n",
      "evaluate: 100%|██████████| 29/29 [00:09<00:00,  2.98it/s]\n",
      "2021-04-29 17:08:17,354 INFO: valid_evaluation: loss=0.13437043143243627, accuracy=0.9425863991081382, precision=0.72, recall=0.801980198019802, f1=0.7587822014051523\n",
      "2021-04-29 17:08:17,360 INFO: Save model at ./models/question_check/question_check_weight5_1_step400_loss0.13437043143243627/\n",
      "2021-04-29 17:08:31,233 INFO: train_loss [500]: 0.20764440320432187\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.84it/s]\n",
      "2021-04-29 17:08:41,463 INFO: valid_evaluation: loss=0.12458874156762814, accuracy=0.9492753623188406, precision=0.7534246575342466, recall=0.8168316831683168, f1=0.7838479809976246\n",
      "2021-04-29 17:08:41,464 INFO: Save model at ./models/question_check/question_check_weight5_1_step500_loss0.12458874156762814/\n",
      "2021-04-29 17:08:54,611 INFO: train_loss [600]: 0.1661454186309129\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.78it/s]\n",
      "2021-04-29 17:09:05,042 INFO: valid_evaluation: loss=0.11284676541028352, accuracy=0.959866220735786, precision=0.8316326530612245, recall=0.806930693069307, f1=0.8190954773869347\n",
      "2021-04-29 17:09:05,043 INFO: Save model at ./models/question_check/question_check_weight5_1_step600_loss0.11284676541028352/\n",
      "2021-04-29 17:09:17,975 INFO: train_loss [700]: 0.12080024337396025\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.88it/s]\n",
      "2021-04-29 17:09:28,048 INFO: valid_evaluation: loss=0.11816487422791021, accuracy=0.9537346711259754, precision=0.7692307692307693, recall=0.8415841584158416, f1=0.8037825059101654\n",
      "2021-04-29 17:09:39,360 INFO: train_loss [800]: 0.12270509463269264\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.80it/s]\n",
      "2021-04-29 17:09:49,733 INFO: valid_evaluation: loss=0.11761567568213775, accuracy=0.9648829431438127, precision=0.8638743455497382, recall=0.8168316831683168, f1=0.8396946564885496\n",
      "2021-04-29 17:09:49,735 INFO: Save model at ./models/question_check/question_check_weight5_1_step800_loss0.11761567568213775/\n",
      "2021-04-29 17:10:03,981 INFO: train_loss [900]: 0.16629192349500954\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.70it/s]\n",
      "2021-04-29 17:10:14,730 INFO: valid_evaluation: loss=0.10863927250792241, accuracy=0.9671125975473801, precision=0.9132947976878613, recall=0.7821782178217822, f1=0.8426666666666666\n",
      "2021-04-29 17:10:14,731 INFO: Save model at ./models/question_check/question_check_weight5_1_step900_loss0.10863927250792241/\n",
      "2021-04-29 17:10:27,836 INFO: train_loss [1000]: 0.12264699895866216\n",
      "evaluate: 100%|██████████| 29/29 [00:09<00:00,  2.90it/s]\n",
      "2021-04-29 17:10:37,831 INFO: valid_evaluation: loss=0.10535898136681524, accuracy=0.9637681159420289, precision=0.9005847953216374, recall=0.7623762376237624, f1=0.8257372654155495\n",
      "2021-04-29 17:10:49,581 INFO: train_loss [1100]: 0.1308779473975301\n",
      "evaluate: 100%|██████████| 29/29 [00:09<00:00,  2.93it/s]\n",
      "2021-04-29 17:10:59,477 INFO: valid_evaluation: loss=0.10416263296943286, accuracy=0.9643255295429208, precision=0.8876404494382022, recall=0.7821782178217822, f1=0.8315789473684211\n",
      "2021-04-29 17:11:11,470 INFO: train_loss [1200]: 0.10527118952944875\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.75it/s]\n",
      "2021-04-29 17:11:22,018 INFO: valid_evaluation: loss=0.11791674012382483, accuracy=0.9643255295429208, precision=0.8876404494382022, recall=0.7821782178217822, f1=0.8315789473684211\n",
      "2021-04-29 17:11:33,633 INFO: train_loss [1300]: 0.0807414242438972\n",
      "evaluate: 100%|██████████| 29/29 [00:09<00:00,  2.97it/s]\n",
      "2021-04-29 17:11:43,404 INFO: valid_evaluation: loss=0.12517629804667726, accuracy=0.9637681159420289, precision=0.9419354838709677, recall=0.7227722772277227, f1=0.8179271708683474\n",
      "2021-04-29 17:11:55,481 INFO: train_loss [1400]: 0.11653465814888478\n",
      "evaluate: 100%|██████████| 29/29 [00:09<00:00,  2.90it/s]\n",
      "2021-04-29 17:12:05,480 INFO: valid_evaluation: loss=0.10685281079776328, accuracy=0.9659977703455964, precision=0.9122807017543859, recall=0.7722772277227723, f1=0.8364611260053618\n",
      "2021-04-29 17:12:16,501 INFO: train_loss [1500]: 0.12060148241464048\n",
      "evaluate: 100%|██████████| 29/29 [00:09<00:00,  2.92it/s]\n",
      "2021-04-29 17:12:26,438 INFO: valid_evaluation: loss=0.1316283856486452, accuracy=0.9548494983277592, precision=0.7510373443983402, recall=0.8960396039603961, f1=0.8171557562076749\n",
      "2021-04-29 17:12:37,740 INFO: train_loss [1600]: 0.13983230956830084\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.72it/s]\n",
      "2021-04-29 17:12:48,403 INFO: valid_evaluation: loss=0.10213435402718084, accuracy=0.9632107023411371, precision=0.8578947368421053, recall=0.806930693069307, f1=0.8316326530612246\n",
      "2021-04-29 17:13:00,699 INFO: train_loss [1700]: 0.132809980744496\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.89it/s]\n",
      "2021-04-29 17:13:10,754 INFO: valid_evaluation: loss=0.1035946302501292, accuracy=0.9648829431438127, precision=0.9064327485380117, recall=0.7673267326732673, f1=0.8310991957104559\n",
      "2021-04-29 17:13:22,855 INFO: train_loss [1800]: 0.1200375811289996\n",
      "evaluate: 100%|██████████| 29/29 [00:09<00:00,  2.99it/s]\n",
      "2021-04-29 17:13:32,553 INFO: valid_evaluation: loss=0.09829793035470206, accuracy=0.9682274247491639, precision=0.9005524861878453, recall=0.806930693069307, f1=0.8511749347258485\n",
      "2021-04-29 17:13:32,555 INFO: Save model at ./models/question_check/question_check_weight5_1_step1800_loss0.09829793035470206/\n",
      "2021-04-29 17:13:45,517 INFO: train_loss [1900]: 0.06305928084068001\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.86it/s]\n",
      "2021-04-29 17:13:55,675 INFO: valid_evaluation: loss=0.09652669124048331, accuracy=0.9687848383500557, precision=0.9055555555555556, recall=0.806930693069307, f1=0.8534031413612566\n",
      "2021-04-29 17:13:55,675 INFO: Save model at ./models/question_check/question_check_weight5_1_step1900_loss0.09652669124048331/\n",
      "2021-04-29 17:14:08,601 INFO: train_loss [2000]: 0.0826534216105938\n",
      "evaluate: 100%|██████████| 29/29 [00:09<00:00,  2.90it/s]\n",
      "2021-04-29 17:14:18,591 INFO: valid_evaluation: loss=0.0966534224370944, accuracy=0.9704570791527313, precision=0.9162011173184358, recall=0.8118811881188119, f1=0.8608923884514436\n",
      "2021-04-29 17:14:18,592 INFO: Save model at ./models/question_check/question_check_weight5_1_step2000_loss0.0966534224370944/\n",
      "2021-04-29 17:14:31,614 INFO: train_loss [2100]: 0.045899132331833244\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.89it/s]\n",
      "2021-04-29 17:14:41,674 INFO: valid_evaluation: loss=0.1001898655667901, accuracy=0.9693422519509476, precision=0.92, recall=0.7970297029702971, f1=0.8541114058355438\n",
      "2021-04-29 17:14:53,097 INFO: train_loss [2200]: 0.07749977413564921\n",
      "evaluate: 100%|██████████| 29/29 [00:11<00:00,  2.63it/s]\n",
      "2021-04-29 17:15:04,111 INFO: valid_evaluation: loss=0.10636968624874435, accuracy=0.9698996655518395, precision=0.9111111111111111, recall=0.8118811881188119, f1=0.8586387434554974\n",
      "2021-04-29 17:15:16,381 INFO: train_loss [2300]: 0.0664236306026578\n",
      "evaluate: 100%|██████████| 29/29 [00:09<00:00,  2.91it/s]\n",
      "2021-04-29 17:15:26,360 INFO: valid_evaluation: loss=0.10156837593892525, accuracy=0.9687848383500557, precision=0.9147727272727273, recall=0.7970297029702971, f1=0.851851851851852\n",
      "2021-04-29 17:15:38,731 INFO: train_loss [2400]: 0.046416182215325535\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.77it/s]\n",
      "2021-04-29 17:15:49,188 INFO: valid_evaluation: loss=0.10414725641623654, accuracy=0.9693422519509476, precision=0.9152542372881356, recall=0.801980198019802, f1=0.8548812664907651\n",
      "2021-04-29 17:16:01,256 INFO: train_loss [2500]: 0.0392489419830963\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.87it/s]\n",
      "2021-04-29 17:16:11,358 INFO: valid_evaluation: loss=0.10299928849241857, accuracy=0.967670011148272, precision=0.9, recall=0.801980198019802, f1=0.8481675392670158\n",
      "2021-04-29 17:16:22,953 INFO: train_loss [2600]: 0.05336701144464314\n",
      "evaluate: 100%|██████████| 29/29 [00:09<00:00,  2.99it/s]\n",
      "2021-04-29 17:16:32,659 INFO: valid_evaluation: loss=0.10669843088193186, accuracy=0.9693422519509476, precision=0.9248554913294798, recall=0.7920792079207921, f1=0.8533333333333334\n",
      "2021-04-29 17:16:44,825 INFO: train_loss [2700]: 0.03897464434616268\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.84it/s]\n",
      "2021-04-29 17:16:55,029 INFO: valid_evaluation: loss=0.11226133925133738, accuracy=0.9687848383500557, precision=0.9195402298850575, recall=0.7920792079207921, f1=0.8510638297872342\n",
      "2021-04-29 17:17:06,679 INFO: train_loss [2800]: 0.05371749216457829\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.86it/s]\n",
      "2021-04-29 17:17:16,836 INFO: valid_evaluation: loss=0.10971154439552076, accuracy=0.9687848383500557, precision=0.9195402298850575, recall=0.7920792079207921, f1=0.8510638297872342\n",
      "2021-04-29 17:17:28,374 INFO: train_loss [2900]: 0.04684982580132782\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.87it/s]\n",
      "2021-04-29 17:17:38,493 INFO: valid_evaluation: loss=0.10590811260044575, accuracy=0.9698996655518395, precision=0.9252873563218391, recall=0.7970297029702971, f1=0.8563829787234043\n",
      "2021-04-29 17:17:51,076 INFO: train_loss [3000]: 0.03135991267859936\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.89it/s]\n",
      "2021-04-29 17:18:01,129 INFO: valid_evaluation: loss=0.10810979452498, accuracy=0.9693422519509476, precision=0.9248554913294798, recall=0.7920792079207921, f1=0.8533333333333334\n",
      "2021-04-29 17:18:12,516 INFO: train_loss [3100]: 0.043996806731447576\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.80it/s]\n",
      "2021-04-29 17:18:22,883 INFO: valid_evaluation: loss=0.10643088904305778, accuracy=0.9698996655518395, precision=0.9252873563218391, recall=0.7970297029702971, f1=0.8563829787234043\n",
      "2021-04-29 17:18:34,739 INFO: train_loss [3200]: 0.052452786494977775\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.76it/s]\n",
      "2021-04-29 17:18:45,235 INFO: valid_evaluation: loss=0.10399544049568217, accuracy=0.9698996655518395, precision=0.9252873563218391, recall=0.7970297029702971, f1=0.8563829787234043\n",
      "2021-04-29 17:18:56,412 INFO: train_loss [3300]: 0.03158202631631866\n",
      "evaluate: 100%|██████████| 29/29 [00:09<00:00,  2.97it/s]\n",
      "2021-04-29 17:19:06,191 INFO: valid_evaluation: loss=0.10762317029052768, accuracy=0.9693422519509476, precision=0.92, recall=0.7970297029702971, f1=0.8541114058355438\n",
      "2021-04-29 17:19:17,627 INFO: train_loss [3400]: 0.03881661350140348\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.71it/s]\n",
      "2021-04-29 17:19:28,330 INFO: valid_evaluation: loss=0.10706111123978064, accuracy=0.9704570791527313, precision=0.9209039548022598, recall=0.806930693069307, f1=0.8601583113456465\n",
      "2021-04-29 17:19:39,961 INFO: train_loss [3500]: 0.035084260038565844\n",
      "evaluate: 100%|██████████| 29/29 [00:09<00:00,  2.91it/s]\n",
      "2021-04-29 17:19:49,932 INFO: valid_evaluation: loss=0.11271086829746592, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:20:02,269 INFO: train_loss [3600]: 0.04527436391450465\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.67it/s]\n",
      "2021-04-29 17:20:13,146 INFO: valid_evaluation: loss=0.15598524650879975, accuracy=0.967670011148272, precision=0.9285714285714286, recall=0.7722772277227723, f1=0.8432432432432433\n",
      "2021-04-29 17:20:24,477 INFO: train_loss [3700]: 0.029700588874984533\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.89it/s]\n",
      "2021-04-29 17:20:34,516 INFO: valid_evaluation: loss=0.11782533219405289, accuracy=0.967670011148272, precision=0.9285714285714286, recall=0.7722772277227723, f1=0.8432432432432433\n",
      "2021-04-29 17:20:45,822 INFO: train_loss [3800]: 0.04759692893130705\n",
      "evaluate: 100%|██████████| 29/29 [00:09<00:00,  2.93it/s]\n",
      "2021-04-29 17:20:55,736 INFO: valid_evaluation: loss=0.1193128539698905, accuracy=0.967670011148272, precision=0.9235294117647059, recall=0.7772277227722773, f1=0.8440860215053764\n",
      "2021-04-29 17:21:06,698 INFO: train_loss [3900]: 0.04083868128247559\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.82it/s]\n",
      "2021-04-29 17:21:16,999 INFO: valid_evaluation: loss=0.11655186174501633, accuracy=0.967670011148272, precision=0.9235294117647059, recall=0.7772277227722773, f1=0.8440860215053764\n",
      "2021-04-29 17:21:28,788 INFO: train_loss [4000]: 0.030033713228767737\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.76it/s]\n",
      "2021-04-29 17:21:39,313 INFO: valid_evaluation: loss=0.1165140036108165, accuracy=0.967670011148272, precision=0.9235294117647059, recall=0.7772277227722773, f1=0.8440860215053764\n",
      "2021-04-29 17:21:51,374 INFO: train_loss [4100]: 0.04878200246952474\n",
      "evaluate: 100%|██████████| 29/29 [00:09<00:00,  2.98it/s]\n",
      "2021-04-29 17:22:01,124 INFO: valid_evaluation: loss=0.11656136993832629, accuracy=0.967670011148272, precision=0.9235294117647059, recall=0.7772277227722773, f1=0.8440860215053764\n",
      "2021-04-29 17:22:13,619 INFO: train_loss [4200]: 0.03197787501150742\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.84it/s]\n",
      "2021-04-29 17:22:23,835 INFO: valid_evaluation: loss=0.11568953000522893, accuracy=0.967670011148272, precision=0.9235294117647059, recall=0.7772277227722773, f1=0.8440860215053764\n",
      "2021-04-29 17:22:34,722 INFO: train_loss [4300]: 0.022288191602565347\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.84it/s]\n",
      "2021-04-29 17:22:44,946 INFO: valid_evaluation: loss=0.12043683230876923, accuracy=0.967670011148272, precision=0.9235294117647059, recall=0.7772277227722773, f1=0.8440860215053764\n",
      "2021-04-29 17:22:56,496 INFO: train_loss [4400]: 0.028317279294133187\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.80it/s]\n",
      "2021-04-29 17:23:06,871 INFO: valid_evaluation: loss=0.11591144757152631, accuracy=0.9682274247491639, precision=0.9239766081871345, recall=0.7821782178217822, f1=0.8471849865951742\n",
      "2021-04-29 17:23:19,581 INFO: train_loss [4500]: 0.026210975588764996\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.89it/s]\n",
      "2021-04-29 17:23:29,626 INFO: valid_evaluation: loss=0.11604386625875687, accuracy=0.9682274247491639, precision=0.9239766081871345, recall=0.7821782178217822, f1=0.8471849865951742\n",
      "2021-04-29 17:23:40,696 INFO: train_loss [4600]: 0.02644654702860862\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.84it/s]\n",
      "2021-04-29 17:23:50,895 INFO: valid_evaluation: loss=0.11636427683948443, accuracy=0.967670011148272, precision=0.9235294117647059, recall=0.7772277227722773, f1=0.8440860215053764\n",
      "2021-04-29 17:24:01,544 INFO: train_loss [4700]: 0.03221494947327301\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.83it/s]\n",
      "2021-04-29 17:24:11,799 INFO: valid_evaluation: loss=0.11582767006395192, accuracy=0.9682274247491639, precision=0.9239766081871345, recall=0.7821782178217822, f1=0.8471849865951742\n",
      "2021-04-29 17:24:24,017 INFO: train_loss [4800]: 0.01912687037605792\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.78it/s]\n",
      "2021-04-29 17:24:34,439 INFO: valid_evaluation: loss=0.11612152924825406, accuracy=0.9682274247491639, precision=0.9239766081871345, recall=0.7821782178217822, f1=0.8471849865951742\n",
      "2021-04-29 17:24:45,748 INFO: train_loss [4900]: 0.022301171869039536\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.86it/s]\n",
      "2021-04-29 17:24:55,886 INFO: valid_evaluation: loss=0.14531688645867438, accuracy=0.9682274247491639, precision=0.9239766081871345, recall=0.7821782178217822, f1=0.8471849865951742\n",
      "2021-04-29 17:25:07,199 INFO: train_loss [5000]: 0.023223131517879664\n",
      "evaluate: 100%|██████████| 29/29 [00:09<00:00,  2.92it/s]\n",
      "2021-04-29 17:25:17,138 INFO: valid_evaluation: loss=0.11683059477343641, accuracy=0.9682274247491639, precision=0.9239766081871345, recall=0.7821782178217822, f1=0.8471849865951742\n",
      "2021-04-29 17:25:29,335 INFO: train_loss [5100]: 0.04166660295566544\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.88it/s]\n",
      "2021-04-29 17:25:39,426 INFO: valid_evaluation: loss=0.1167208116565799, accuracy=0.9682274247491639, precision=0.9239766081871345, recall=0.7821782178217822, f1=0.8471849865951742\n",
      "2021-04-29 17:25:50,537 INFO: train_loss [5200]: 0.056142391331959515\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.79it/s]\n",
      "2021-04-29 17:26:00,924 INFO: valid_evaluation: loss=0.11480234759249563, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:26:12,422 INFO: train_loss [5300]: 0.035417182040400806\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.88it/s]\n",
      "2021-04-29 17:26:22,514 INFO: valid_evaluation: loss=0.11467886915237739, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:26:35,107 INFO: train_loss [5400]: 0.03651116369990632\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.88it/s]\n",
      "2021-04-29 17:26:45,183 INFO: valid_evaluation: loss=0.1155088971336854, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:26:56,291 INFO: train_loss [5500]: 0.020297516584396363\n",
      "evaluate: 100%|██████████| 29/29 [00:09<00:00,  2.90it/s]\n",
      "2021-04-29 17:27:06,286 INFO: valid_evaluation: loss=0.11557844621611052, accuracy=0.9682274247491639, precision=0.9239766081871345, recall=0.7821782178217822, f1=0.8471849865951742\n",
      "2021-04-29 17:27:18,708 INFO: train_loss [5600]: 0.025350150340236723\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.72it/s]\n",
      "2021-04-29 17:27:29,388 INFO: valid_evaluation: loss=0.3558015142856487, accuracy=0.9682274247491639, precision=0.9239766081871345, recall=0.7821782178217822, f1=0.8471849865951742\n",
      "2021-04-29 17:27:41,336 INFO: train_loss [5700]: 0.034460380233358595\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.74it/s]\n",
      "2021-04-29 17:27:51,931 INFO: valid_evaluation: loss=0.11554414493128143, accuracy=0.9682274247491639, precision=0.9239766081871345, recall=0.7821782178217822, f1=0.8471849865951742\n",
      "2021-04-29 17:28:02,674 INFO: train_loss [5800]: 0.044526435957523064\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.88it/s]\n",
      "2021-04-29 17:28:12,749 INFO: valid_evaluation: loss=0.11591316810969648, accuracy=0.9682274247491639, precision=0.9239766081871345, recall=0.7821782178217822, f1=0.8471849865951742\n",
      "2021-04-29 17:28:24,035 INFO: train_loss [5900]: 0.0335911735589616\n",
      "evaluate: 100%|██████████| 29/29 [00:09<00:00,  2.95it/s]\n",
      "2021-04-29 17:28:33,875 INFO: valid_evaluation: loss=0.11542263110007706, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:28:45,668 INFO: train_loss [6000]: 0.026102181083988398\n",
      "evaluate: 100%|██████████| 29/29 [00:09<00:00,  2.96it/s]\n",
      "2021-04-29 17:28:55,456 INFO: valid_evaluation: loss=0.11544632173046984, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:29:07,393 INFO: train_loss [6100]: 0.021262510435190052\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.85it/s]\n",
      "2021-04-29 17:29:17,592 INFO: valid_evaluation: loss=0.11549688320093114, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:29:28,963 INFO: train_loss [6200]: 0.03921846505603753\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.79it/s]\n",
      "2021-04-29 17:29:39,350 INFO: valid_evaluation: loss=0.11553391888095386, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:29:52,121 INFO: train_loss [6300]: 0.02677620162139647\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.88it/s]\n",
      "2021-04-29 17:30:02,211 INFO: valid_evaluation: loss=0.11551181506365538, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:30:13,724 INFO: train_loss [6400]: 0.04387863976880908\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.79it/s]\n",
      "2021-04-29 17:30:24,104 INFO: valid_evaluation: loss=0.11559087288533819, accuracy=0.9682274247491639, precision=0.9239766081871345, recall=0.7821782178217822, f1=0.8471849865951742\n",
      "2021-04-29 17:30:34,922 INFO: train_loss [6500]: 0.03659599459031597\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.85it/s]\n",
      "2021-04-29 17:30:45,110 INFO: valid_evaluation: loss=0.1674483536646284, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:30:56,778 INFO: train_loss [6600]: 0.035416840296238664\n",
      "evaluate: 100%|██████████| 29/29 [00:09<00:00,  2.92it/s]\n",
      "2021-04-29 17:31:06,707 INFO: valid_evaluation: loss=0.11547439866538706, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:31:19,055 INFO: train_loss [6700]: 0.03571129125892185\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.87it/s]\n",
      "2021-04-29 17:31:29,168 INFO: valid_evaluation: loss=0.11542477298142581, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:31:40,869 INFO: train_loss [6800]: 0.025275913316290824\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.83it/s]\n",
      "2021-04-29 17:31:51,117 INFO: valid_evaluation: loss=0.11544121352249179, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:32:02,229 INFO: train_loss [6900]: 0.01984976537525654\n",
      "evaluate: 100%|██████████| 29/29 [00:09<00:00,  2.97it/s]\n",
      "2021-04-29 17:32:12,012 INFO: valid_evaluation: loss=0.11600354108317144, accuracy=0.9682274247491639, precision=0.9239766081871345, recall=0.7821782178217822, f1=0.8471849865951742\n",
      "2021-04-29 17:32:23,840 INFO: train_loss [7000]: 0.0430983562534675\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.76it/s]\n",
      "2021-04-29 17:32:34,367 INFO: valid_evaluation: loss=0.11536126018598161, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:32:46,490 INFO: train_loss [7100]: 0.03081593704177067\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.77it/s]\n",
      "2021-04-29 17:32:56,956 INFO: valid_evaluation: loss=0.11558332136864292, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:33:09,075 INFO: train_loss [7200]: 0.025186192775145173\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.79it/s]\n",
      "2021-04-29 17:33:19,474 INFO: valid_evaluation: loss=0.16683454670268913, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:33:31,117 INFO: train_loss [7300]: 0.04931967057287693\n",
      "evaluate: 100%|██████████| 29/29 [00:09<00:00,  2.93it/s]\n",
      "2021-04-29 17:33:41,007 INFO: valid_evaluation: loss=0.11570777040745678, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:33:52,436 INFO: train_loss [7400]: 0.02391190586378798\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.73it/s]\n",
      "2021-04-29 17:34:03,076 INFO: valid_evaluation: loss=0.11536704578661713, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:34:14,846 INFO: train_loss [7500]: 0.017225139271467925\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.84it/s]\n",
      "2021-04-29 17:34:25,068 INFO: valid_evaluation: loss=0.11536655870491061, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:34:37,202 INFO: train_loss [7600]: 0.04146993587142788\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.87it/s]\n",
      "2021-04-29 17:34:47,316 INFO: valid_evaluation: loss=0.11537934826879666, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:34:58,822 INFO: train_loss [7700]: 0.028363939041737465\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.73it/s]\n",
      "2021-04-29 17:35:09,458 INFO: valid_evaluation: loss=0.11594262064017098, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:35:20,813 INFO: train_loss [7800]: 0.029481618995778263\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.82it/s]\n",
      "2021-04-29 17:35:31,104 INFO: valid_evaluation: loss=0.11537028992034752, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:35:42,688 INFO: train_loss [7900]: 0.022922509964555502\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.75it/s]\n",
      "2021-04-29 17:35:53,239 INFO: valid_evaluation: loss=0.11536870973891225, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:36:05,348 INFO: train_loss [8000]: 0.03773163423407823\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.65it/s]\n",
      "2021-04-29 17:36:16,310 INFO: valid_evaluation: loss=0.11537305814824228, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:36:28,999 INFO: train_loss [8100]: 0.021982419395353645\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.88it/s]\n",
      "2021-04-29 17:36:39,076 INFO: valid_evaluation: loss=0.11537099099750149, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:36:50,493 INFO: train_loss [8200]: 0.0441398154059425\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.78it/s]\n",
      "2021-04-29 17:37:00,931 INFO: valid_evaluation: loss=0.1153636037950115, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:37:17,343 INFO: train_loss [8300]: 0.042690015439875426\n",
      "evaluate: 100%|██████████| 29/29 [00:12<00:00,  2.41it/s]\n",
      "2021-04-29 17:37:29,394 INFO: valid_evaluation: loss=0.1153756755500518, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:37:48,281 INFO: train_loss [8400]: 0.03196457794634625\n",
      "evaluate: 100%|██████████| 29/29 [00:12<00:00,  2.31it/s]\n",
      "2021-04-29 17:38:00,850 INFO: valid_evaluation: loss=0.115363085192853, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:38:19,462 INFO: train_loss [8500]: 0.02485085939290002\n",
      "evaluate: 100%|██████████| 29/29 [00:12<00:00,  2.31it/s]\n",
      "2021-04-29 17:38:32,022 INFO: valid_evaluation: loss=0.11539807305510702, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:38:49,580 INFO: train_loss [8600]: 0.034636504440568386\n",
      "evaluate: 100%|██████████| 29/29 [00:12<00:00,  2.36it/s]\n",
      "2021-04-29 17:39:01,874 INFO: valid_evaluation: loss=0.22131576128946295, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:39:20,395 INFO: train_loss [8700]: 0.02251075838226825\n",
      "evaluate: 100%|██████████| 29/29 [00:12<00:00,  2.25it/s]\n",
      "2021-04-29 17:39:33,328 INFO: valid_evaluation: loss=0.11538958729341112, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:39:51,791 INFO: train_loss [8800]: 0.041406232395675036\n",
      "evaluate: 100%|██████████| 29/29 [00:12<00:00,  2.29it/s]\n",
      "2021-04-29 17:40:04,494 INFO: valid_evaluation: loss=0.11536515118746922, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:40:23,765 INFO: train_loss [8900]: 0.02028082008473575\n",
      "evaluate: 100%|██████████| 29/29 [00:12<00:00,  2.34it/s]\n",
      "2021-04-29 17:40:36,206 INFO: valid_evaluation: loss=0.11540885597210505, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:40:51,845 INFO: train_loss [9000]: 0.04227062695194036\n",
      "evaluate: 100%|██████████| 29/29 [00:09<00:00,  2.92it/s]\n",
      "2021-04-29 17:41:01,790 INFO: valid_evaluation: loss=0.11538951847188432, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:41:13,038 INFO: train_loss [9100]: 0.02314704971620813\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.75it/s]\n",
      "2021-04-29 17:41:23,590 INFO: valid_evaluation: loss=0.11535956187109495, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:41:35,470 INFO: train_loss [9200]: 0.02266461806022562\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.85it/s]\n",
      "2021-04-29 17:41:45,658 INFO: valid_evaluation: loss=0.11626431111503265, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:41:57,030 INFO: train_loss [9300]: 0.0487420245539397\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.77it/s]\n",
      "2021-04-29 17:42:07,494 INFO: valid_evaluation: loss=0.115360367577523, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:42:19,168 INFO: train_loss [9400]: 0.02297362564248033\n",
      "evaluate: 100%|██████████| 29/29 [00:10<00:00,  2.79it/s]\n",
      "2021-04-29 17:42:29,556 INFO: valid_evaluation: loss=0.11536002069197852, accuracy=0.9687848383500557, precision=0.9244186046511628, recall=0.7871287128712872, f1=0.8502673796791445\n",
      "2021-04-29 17:42:41,236 INFO: train_loss [9500]: 0.028346707174787298\n",
      "evaluate:  48%|████▊     | 14/29 [00:06<00:07,  2.09it/s]\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ffdebd411809>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-5318c24fe8aa>\u001b[0m in \u001b[0;36mbert_main\u001b[0;34m(texts, trues, model_dir, save_dir)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mshow_eval_per_iter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mevaluation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 logging.info('valid_evaluation: loss={loss}, accuracy={accuracy}, '\n\u001b[1;32m     94\u001b[0m                              \u001b[0;34m'precision={precision}, recall={recall}, f1={f1}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-802e60ae24cc>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, valid_loader)\u001b[0m\n\u001b[1;32m     36\u001b[0m             )\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mloss_averager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mcorrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bert_main(texts, trues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 測試結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(path):\n",
    "    df = pd.read_csv(path, sep='\\t')\n",
    "    \n",
    "    texts, trues = [], []\n",
    "    for i in df.index:\n",
    "        question = df.loc[i, 'question']\n",
    "        answer = df.loc[i, 'answer']\n",
    "        \n",
    "        question = ''.join([q for q in question if re.match(CN_EN_RE, q)])\n",
    "        answer = ''.join([a for a in answer if re.match(CN_EN_RE, a)])\n",
    "\n",
    "        texts.append([question, answer])\n",
    "        label = 1 if df.loc[i, 'label'] == 5 else 0\n",
    "        trues.append(label)\n",
    "    return texts, trues\n",
    "\n",
    "path = '/home/jovyan/wm-insur-call-qa/owen/data/qa_raw_copy.tsv'\n",
    "texts, trues = prepare_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4282bd142ffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#     tokens, token_types = tokenizer.tokenize(text[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mkeys_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": [
    "# 實驗1 - 自定義辭庫抓關鍵字\n",
    "keys = '本人|解約|保費|健康|親自|日常支出|申請|說明|字號|風險|來源|繳費|終止|用途|後三碼|末三碼|簽名|錄音|\\\n",
    "電話|不當行銷|搭售|出生|先生|小姐|電訪|訪問|告知事項|風險|不當話術|不影響|受益人|知道|正確|變更|權利|義務|\\\n",
    "了解|確定|同意|清楚|收取|文件|恢復|協助|表單|一次繳清|躉繳|確認|投保|薪資|土地|帳戶|保單|管理|投資|貸款|房貸|房租|利息|收入\\\n",
    "年|月|日|保險|條款|照會|方便|帳單|股票|基金|投資|核對|辦理|契約'\n",
    "tmp_texts = []\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "for text in texts:\n",
    "#     tokens, token_types = tokenizer.tokenize(text[0])\n",
    "    keys_lst = keys.split('|')\n",
    "#     text_keyword = [token for token in tokens if token in keys_lst]\n",
    "    text_keyword = [k for k in keys_lst if re.search(k, text[0])]\n",
    "#     if not text_keyword:\n",
    "#         print(text, text_keyword)\n",
    "    tmp_texts.append([text[1]] + text_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/jovyan/.local/lib/python3.6/site-packages/esun_phoneme_tool/jieba_dict_pool/general_jieba_0002.txt ...\n",
      "2021-04-28 16:03:28,089 DEBUG: Building prefix dict from /home/jovyan/.local/lib/python3.6/site-packages/esun_phoneme_tool/jieba_dict_pool/general_jieba_0002.txt ...\n",
      "Loading model from cache /tmp/jieba.u9c666b2c1c48c114768336a4722d3b92.cache\n",
      "2021-04-28 16:03:28,090 DEBUG: Loading model from cache /tmp/jieba.u9c666b2c1c48c114768336a4722d3b92.cache\n",
      "Loading model cost 0.658 seconds.\n",
      "2021-04-28 16:03:28,748 DEBUG: Loading model cost 0.658 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "2021-04-28 16:03:28,749 DEBUG: Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[['小姐'], ['本人'], ['請問'], ['一下'], ['禮儀']],\n",
       " [['江淑'], ['方便'], ['訪問'], ['電話'], ['小姐']],\n",
       " [['江淑'], ['小姐'], ['本人'], ['請問'], ['福盛']],\n",
       " [['憑證'], ['保證'], ['繳交'], ['近期'], ['透過']],\n",
       " [['林建銘'], ['先生'], ['本人'], ['請問'], ['福利']],\n",
       " [['萬能'], ['大智'], ['變額'], ['巴黎'], ['法國']],\n",
       " [['投資型'], ['淨值'], ['方面'], ['保證'], ['變動']],\n",
       " [['天母'], ['親自'], ['藉由'], ['本人'], ['健康']],\n",
       " [['參考'], ['成本'], ['管理費'], ['進入'], ['條款']],\n",
       " [['扣掉'], ['解約'], ['提前'], ['當年度'], ['當時']],\n",
       " [['繳交'], ['來源'], ['資金'], ['解約'], ['保費']],\n",
       " [['還是'], ['保費'], ['就是'], ['繳交'], ['來源']],\n",
       " [['詢問'], ['儲蓄'], ['一次'], ['繳交'], ['來源']],\n",
       " [['規劃'], ['確實'], ['付費'], ['綜合'], ['考量']],\n",
       " [['都會'], ['提領'], ['近期'], ['透過'], ['部分']],\n",
       " [['林森'], ['林冠廷'], ['藉由'], ['提領'], ['這次']],\n",
       " [['提領'], ['部分'], ['契約'], ['終止'], ['以及']],\n",
       " [['提領'], ['無法'], ['一次'], ['原本'], ['恢復']],\n",
       " [['小姐'], ['本人'], ['請問'], ['一下'], ['禮儀']],\n",
       " [['近期'], ['透過'], ['依照'], ['感謝'], ['稍後']],\n",
       " [['安聯'], ['號碼'], ['收到'], ['接下來'], ['問題']],\n",
       " [['城東'], ['藉由'], ['這次'], ['要保人'], ['旁邊']],\n",
       " [['為何'], ['用途'], ['方便'], ['這次'], ['資金']],\n",
       " [['中止'], ['方式'], ['了解'], ['確認'], ['正確']],\n",
       " [['受益人'], ['變更'], ['近期'], ['透過'], ['依照']],\n",
       " [['受益人'], ['王玉芬'], ['變更'], ['女兒'], ['嘉義']],\n",
       " [['泓泉'], ['先生'], ['本人'], ['請問'], ['福利']],\n",
       " [['近期'], ['透過'], ['依照'], ['感謝'], ['稍後']],\n",
       " [['安聯'], ['號碼'], ['收到'], ['接下來'], ['問題']],\n",
       " [['藉由'], ['這次'], ['要保人'], ['分行'], ['文件']],\n",
       " [['為何'], ['用途'], ['方便'], ['這次'], ['資金']],\n",
       " [['正確'], ['並未'], ['鼓勵'], ['勸誘'], ['人員']],\n",
       " [['沒有'], ['鼓勵'], ['勸誘'], ['人員'], ['本行']],\n",
       " [['無法'], ['原本'], ['恢復'], ['契約'], ['知道']],\n",
       " [['先生'], ['本人'], ['請問'], ['福利'], ['福盛']],\n",
       " [['近期'], ['透過'], ['依照'], ['感謝'], ['稍後']],\n",
       " [['安聯'], ['號碼'], ['收到'], ['接下來'], ['問題']],\n",
       " [['埔墘'], ['這次'], ['要保人'], ['旁邊'], ['分行']],\n",
       " [['為何'], ['用途'], ['方便'], ['這次'], ['資金']],\n",
       " [['正確'], ['並未'], ['鼓勵'], ['勸誘'], ['人員']],\n",
       " [['就是'], ['一下'], ['並未'], ['鼓勵'], ['勸誘']],\n",
       " [['無法'], ['原本'], ['恢復'], ['契約'], ['知道']],\n",
       " [['小姐'], ['本人'], ['請問'], ['一下'], ['禮儀']],\n",
       " [['威力'], ['每年'], ['還本'], ['終生'], ['大約']],\n",
       " [['文件'], ['海外'], ['海外股票'], ['人生'], ['清單']],\n",
       " [['收取'], ['林郁婷'], ['等級'], ['表單'], ['承受']],\n",
       " [['匯率'], ['外幣'], ['變動'], ['自行'], ['存款']],\n",
       " [['就是'], ['新莊'], ['本人'], ['被保險人'], ['親自']],\n",
       " [['儲蓄'], ['付款'], ['理專'], ['影響'], ['目的']],\n",
       " [['保單'], ['借款'], ['解約'], ['風險'], ['財務']],\n",
       " [['美英'], ['小姐'], ['本人'], ['請問'], ['一下']],\n",
       " [['近期'], ['透過'], ['依照'], ['感謝'], ['稍後']],\n",
       " [['事情'], ['號碼'], ['收到'], ['接下來'], ['申請']],\n",
       " [['藉由'], ['這次'], ['要保人'], ['旁邊'], ['分行']],\n",
       " [['為何'], ['用途'], ['方便'], ['這次'], ['資金']],\n",
       " [['無法'], ['原本'], ['恢復'], ['契約'], ['知道']],\n",
       " [['鄭立'], ['小姐'], ['本人'], ['請問'], ['祝壽']],\n",
       " [['近期'], ['透過'], ['感謝'], ['萬能'], ['本行']],\n",
       " [['藉由'], ['中原'], ['親自'], ['健康'], ['填寫']],\n",
       " [['參考'], ['條款'], ['理專'], ['提供'], ['符合']],\n",
       " [['成本'], ['管理費'], ['進入'], ['或是'], ['扣除']],\n",
       " [['然後'], ['解約'], ['提前'], ['受理'], ['中途']],\n",
       " [['放一段'], ['聆聽'], ['掛斷'], ['不要'], ['仔細']],\n",
       " [['保單'], ['風險'], ['解約'], ['損失'], ['借款']],\n",
       " [['掰掰'], ['提醒'], ['事事'], ['耐心'], ['收到']],\n",
       " [['小姐'], ['本人'], ['請問'], ['一下'], ['禮儀']],\n",
       " [['地址'], ['變更'], ['近期'], ['透過'], ['依照']],\n",
       " [['地址'], ['變更'], ['號碼'], ['收到'], ['接下來']],\n",
       " [['岡山'], ['地址'], ['變更'], ['這次'], ['要保人']],\n",
       " [['陳素'], ['小姐'], ['本人'], ['請問'], ['福利']],\n",
       " [['近期'], ['透過'], ['依照'], ['感謝'], ['稍後']],\n",
       " [['安聯'], ['號碼'], ['收到'], ['接下來'], ['問題']],\n",
       " [['古亭'], ['藉由'], ['這次'], ['要保人'], ['旁邊']],\n",
       " [['為何'], ['用途'], ['方便'], ['這次'], ['資金']],\n",
       " [['無法'], ['原本'], ['恢復'], ['契約'], ['知道']],\n",
       " [['小姐'], ['本人'], ['請問'], ['一下'], ['禮儀']],\n",
       " [['憑條'], ['繳交'], ['近期'], ['透過'], ['依照']],\n",
       " [['訪問'], ['同意'], ['電話'], ['全程'], ['錄音']],\n",
       " [['訪問'], ['同意'], ['電話'], ['全程'], ['錄音']],\n",
       " [['林建銘'], ['先生'], ['本人'], ['請問'], ['福利']],\n",
       " [['最近'], ['萬能'], ['大智'], ['變額'], ['法國']],\n",
       " [['投資型'], ['淨值'], ['方面'], ['保證'], ['變動']],\n",
       " [['藉由'], ['天母'], ['親自'], ['健康'], ['填寫']],\n",
       " [['參考'], ['條款'], ['理專'], ['提供'], ['符合']],\n",
       " [['成本'], ['管理費'], ['進入'], ['或是'], ['扣除']],\n",
       " [['解約'], ['提前'], ['受理'], ['中途'], ['當年度']],\n",
       " [['繳交'], ['來源'], ['資金'], ['解約'], ['保費']],\n",
       " [['規劃'], ['確實'], ['再次'], ['付費'], ['綜合']],\n",
       " [['陳文智'], ['桃園'], ['先生'], ['本人'], ['請問']],\n",
       " [['些許'], ['房貸'], ['耽誤'], ['照會'], ['電訪']],\n",
       " [['進行'], ['滿屋'], ['為了'], ['幸福'], ['定期']],\n",
       " [['接著'], ['請教'], ['如果'], ['存款'], ['領回']],\n",
       " [['藉由'], ['藝文'], ['先生'], ['親自'], ['桃園']],\n",
       " [['藝文'], ['接著'], ['請教'], ['符合'], ['先生']],\n",
       " [['接著'], ['繳清'], ['招攬'], ['一次'], ['請教']],\n",
       " [['接著'], ['請教'], ['繳交'], ['來源'], ['先生']],\n",
       " [['保單'], ['風險'], ['請教'], ['解約'], ['損失']],\n",
       " [['經過'], ['不會'], ['規劃'], ['確實'], ['付費']],\n",
       " [['沒有'], ['搭售'], ['不當'], ['情形'], ['行銷']],\n",
       " [['秀嬌'], ['小姐'], ['本人'], ['請問'], ['一下']]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 實驗2 - tf-idf\n",
    "tokenizer = Tokenizer()\n",
    "stop_words = '玉山|玉山銀行|保代|保代部|保險代理部|您好|你好|這裡|這邊|之後|打擾|敝姓|員工|編號|員編|總行|消金|中心|不好意思|呃|嗯'\n",
    "\n",
    "# clean stop words\n",
    "token_texts = []\n",
    "for text in texts:\n",
    "    tokens, token_types = tokenizer.tokenize(text[0])\n",
    "    tmp_tokens = []\n",
    "    for token, token_type in zip(tokens, token_types):\n",
    "        if token_type == 'CN' and token not in stop_words.split('|'):\n",
    "            tmp_tokens.append(token)\n",
    "    token_texts.append(' '.join(tmp_tokens))\n",
    "\n",
    "# get top 5 token from tf-idf\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "tfidf_mat = tfidf_vec.fit_transform(token_texts,)\n",
    "vocab_dict =tfidf_vec.vocabulary_#获得所有文本的关键字和其位置的dict\n",
    "weight = tfidf_mat.toarray()\n",
    "feat = np.argsort(-weight)#降序排序\n",
    "total_key_word = []\n",
    "for l in range(len(token_texts)):\n",
    "    values_word = []\n",
    "    for j in range(5):#获取每类文本的5个关键字\n",
    "        values_word.append([k for k,v in vocab_dict.items() if v == feat[l,j]])\n",
    "    total_key_word.append(values_word)\n",
    "total_key_word[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "q_tokens = [list(chain(*sentence)) for sentence in total_key_word]\n",
    "processed_texts = [q_token + text for q_token, text in zip(q_tokens, texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['小姐',\n",
       "  '本人',\n",
       "  '請問',\n",
       "  '一下',\n",
       "  '禮儀',\n",
       "  '您好這裡是玉山銀行您好這裡是玉山銀行保險代理部敝姓唐員工編號請問是湯拱運儲小姐本人嗎您好',\n",
       "  '喂喂喂'],\n",
       " ['江淑',\n",
       "  '方便',\n",
       "  '訪問',\n",
       "  '電話',\n",
       "  '小姐',\n",
       "  '呃您好這邊是玉山銀行總行保險代理部敝姓唐員工編號請問是江淑華小姐本人嗎呃呃您好請問她在嗎方便跟他做個電話訪問嗎',\n",
       "  '喔謝謝'],\n",
       " ['江淑',\n",
       "  '小姐',\n",
       "  '本人',\n",
       "  '請問',\n",
       "  '福盛',\n",
       "  '您好呃您好這邊是玉山銀行總行保險代理部敝姓唐員工編號請問是江淑華小姐本人嗎',\n",
       "  '嘿您好對'],\n",
       " ['憑證',\n",
       "  '保證',\n",
       "  '繳交',\n",
       "  '近期',\n",
       "  '透過',\n",
       "  '呃您好不好意思打擾您感謝您近期透過本行辦理首期保費繳交憑證那依照保險法令的要求為保證您的權益稍後電話訪問的內容會全程錄音請問您同意嗎',\n",
       "  '好可可以'],\n",
       " ['林建銘', '先生', '本人', '請問', '福利', '您好這裡是玉山銀行總行保代部敝姓張員工編號請問是林建銘先生本人嗎', '喂嘿是'],\n",
       " ['萬能',\n",
       "  '大智',\n",
       "  '變額',\n",
       "  '巴黎',\n",
       "  '法國',\n",
       "  '呃您好感謝您近期透過本行投保法國巴黎人壽大智富變額萬能壽險繳費年期為輪繳那依照保險法令的要求為保障您的權益稍後電話訪問內容將會全程錄音請問您同意嗎',\n",
       "  '好可以'],\n",
       " ['投資型',\n",
       "  '淨值',\n",
       "  '方面',\n",
       "  '保證',\n",
       "  '變動',\n",
       "  '好謝謝您那請問您是否知道本次購買的是法國巴黎人壽的投資型保險須自行承擔淨值變動的風險並無投資方面的保證呢',\n",
       "  '喔是了解'],\n",
       " ['天母',\n",
       "  '親自',\n",
       "  '藉由',\n",
       "  '本人',\n",
       "  '健康',\n",
       "  '好的那您是否是藉由天母分行的陳若期在旁邊協助並由要被保險您本人親自簽名且被保險人健康告知事項由您本人親自填寫呢',\n",
       "  '嘿是的'],\n",
       " ['參考',\n",
       "  '成本',\n",
       "  '管理費',\n",
       "  '進入',\n",
       "  '條款',\n",
       "  '好的那請問理專是否有提供保險條款給您參考並說明產品內容來確認符合您的需求呢好的那請問您是否知道您繳的保費會扣除相關費用後像是保單管理費或是保險成本等才進入投資帳戶呢',\n",
       "  '有的是知道'],\n",
       " ['扣掉',\n",
       "  '解約',\n",
       "  '提前',\n",
       "  '當年度',\n",
       "  '當時',\n",
       "  '好的那請問您是否知道如果中提部分提領或是提前解約保險公司會依照您當時的保單帳戶價值扣掉當年度的解約費用後在給付給您呢',\n",
       "  '嘿知道']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['您好請問是先生小姐本人嗎',\n",
       " '您好這裡是玉山銀行總行保代部分行敝姓員工編號請問是先生小姐本人嗎',\n",
       " '請問您的出生年月日是',\n",
       " '請問您知道本次購買的人壽保險不是存款如果辦理解約將可能只領回部分已繳保費',\n",
       " '請問您投保時是由分行的從旁協助並由您本人親自填寫健康告知事項及簽名的嗎',\n",
       " '請問您投保時是由分行的從旁協助並由要保人及被保險人親自簽名且被保險人的健康告知事項都是親自填寫的嗎',\n",
       " '招攬人員有向您說明產品內容且符合您的保險需求繳交保費並不影響您的日常支出請問正確嗎',\n",
       " '接下來會為您撥放一段語音宣告請先不要掛斷電話本行人員並未鼓勵或勸誘以辦理貸款保單借款定存解約或保單解約保單終止之方式從事投資理財或購買保險若有上述資金運用於本次產品須事先審慎評估自身財務狀況與風險承受能力並願承擔因財務槓桿操作方式所面臨的風險及保單轉投保之權益損失除辦理貸款或保單借款需支付本金及利息外該產品可能發生之相關風險及最大可能損失請問您是否都瞭解呢',\n",
       " '為維護您的資料安全這裡簡單跟您核對基本資料您的身分證字號是AXXX請問後三碼是',\n",
       " '請問您是否知道本次購買的是人壽的投資型保險需自行承擔淨值變動的風險並無投資方面的保證',\n",
       " '請問您投保時是否皆由分行的在旁邊協助並由要保人及被保險人親自簽名且被保險人之健康告知事項皆由被保險人確認後親自填寫',\n",
       " '請問您投保時是否皆由分行的在旁邊協助並由法定代理人您協助要保人及被保險人親自簽名且被保險人之健康告知事項皆由被保險人之法定代理人確認後親自填寫',\n",
       " '請問法定代理人欄位是否由您本人親自簽名',\n",
       " '請問您投保時的各項文件是否皆由分行的在旁邊協助簽名欄位是否由您親自蓋手印確認且被保險人之健康告知事項皆由被保險人確認後親自填寫',\n",
       " '請問理專是否有提供保險條款給您參考並說明產品內容來確認符合您的需求',\n",
       " '請問您是否知道您所繳的保費會扣除相關費用後才進入投資帳戶',\n",
       " '請問您是否知道如果中途部分提領或提前解約保險公司會依受理當時的保單帳戶價值扣除當年度的解約費用後再給付給您',\n",
       " '請問您本次投保繳交保費的資金來源是否為',\n",
       " '請問您是否已事先審慎評估自身財務狀況與風險承受能力並願承擔因財務槓桿操作方式所面臨的風險及辦理保單解約轉投保之權益損失除辦理貸款或保單借款需支付本金及利息外還有該產品可能發生之相關風險及最大可能損失且本行人員並未鼓勵或勸誘以辦理貸款保單借款保單解約保單終止及定存解約之方式購買保險請問您是否已瞭解',\n",
       " '再次與您確認本保單之規劃您是否已確實瞭解投保目的保險需求並經綜合考量財務狀況以及付費能力且不影響您的日常支出',\n",
       " '請問分行的是否有向您說明產品內容並確認符合您的需求',\n",
       " '請問兩位見證人是否皆由他們分別親自簽名',\n",
       " '感謝您近期透過本行投保人壽繳費年期為年躉繳依照保險法令的要求為保障您的權益稍後電話訪問內容將會全程錄音請問您同意嗎',\n",
       " '請問您是否知道本次購買的是人壽的外幣保險不是存款須自行承擔匯率變動的風險',\n",
       " '請問您是否知道如果辦理解約將可能只能領回部分已繳保費',\n",
       " '請問您投保時是否皆由消金中心的在旁邊協助並由您本人親自簽名且被保險人之健康告知事項皆由您確認後親自填寫',\n",
       " '請問招攬人員是否有提供您躉繳一次繳清與分期繳等不同繳費方式選擇',\n",
       " '與您再次確認上述投保內容和本次貸款並沒有搭售或不當行銷的情形發生請問是否正確',\n",
       " '是否由顧客自行更新網路銀行帳號與密碼',\n",
       " '請問您本次辦理貸款及保險是否有新申請玉山網路銀行',\n",
       " '是否皆由您本人親自更新網路銀行的帳號與密碼',\n",
       " '請問您是否知道辦理契約終止後就無法再恢復成原本的保單內容呢',\n",
       " '本次的保費資金來源是否為']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 實驗3 - 標準話術權威控制\n",
    "with open('phrase.txt', 'r') as fr:\n",
    "    lines = fr.readlines()\n",
    "    \n",
    "    lines = [line.strip('\\n').strip('\\t') for line in lines]\n",
    "    tmp = []\n",
    "    for line in lines:\n",
    "        line = line.strip('\\n').strip('\\t')\n",
    "        line = ''.join([l for l in line if re.match(CN_EN_RE, l)])\n",
    "        if line:\n",
    "            tmp.append(line)\n",
    "lines = tmp\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8966"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count diff ratio and replace sentence\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(str1, str2):\n",
    "    return SequenceMatcher(None, str1, str2).ratio()\n",
    "\n",
    "tmp_texts = []\n",
    "for text in texts:   \n",
    "    diff_ratio = [similar(text[0], line) for line in lines]\n",
    "    if max(diff_ratio) > 0.70:\n",
    "        simi_sentence = lines[np.argmax(diff_ratio)]\n",
    "        tmp_texts.append([simi_sentence, text[1]])\n",
    "    else:\n",
    "        tmp_texts.append([text[0], text[1]])\n",
    "            #         print(df.loc[i, 'question'])\n",
    "#     df.loc[i, 'question_x'] = simi_sentence\n",
    "len(tmp_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用交集判斷相似\n",
    "questions = []\n",
    "for line in lines:\n",
    "    line, _ = tokenizer.tokenize(line)\n",
    "    questions.append(set(line))\n",
    "\n",
    "df = pd.DataFrame()\n",
    "tmp_texts = []\n",
    "for text in texts:\n",
    "    token, _ = tokenizer.tokenize(text[0])\n",
    "    token = set(token)\n",
    "    diff_ratio = [len(q & token) / len(q) for q in questions]\n",
    "    \n",
    "#     simi_sentence = lines[np.argmax(diff_ratio)]\n",
    "#     df = df.append({'text':text[0], 'simi':simi_sentence, 'ratio':max(diff_ratio)}, ignore_index=True)\n",
    "    \n",
    "    if max(diff_ratio) > 0.60:\n",
    "        simi_sentence = lines[np.argmax(diff_ratio)]\n",
    "        tmp_texts.append([simi_sentence, text[1]])\n",
    "    else:\n",
    "        tmp_texts.append([text[0], text[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratio</th>\n",
       "      <th>simi</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>請問法定代理人欄位是否由您本人親自簽名</td>\n",
       "      <td>好的請問您這次辦理契約終止以及部分提領的文件是否藉由林森分行的林冠廷在旁邊協助並由要保人您親...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>請問您是否知道辦理契約終止後就無法再恢復成原本的保單內容呢</td>\n",
       "      <td>好的謝謝您接下來幾項問題項您確認我們有收到您安聯人壽保單號碼ql的契約終止文件一份請問您是否...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>請問您是否知道辦理契約終止後就無法再恢復成原本的保單內容呢</td>\n",
       "      <td>好的好的方便問您這次辦理契約終止的資金用途為何呢</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.388889</td>\n",
       "      <td>請問您是否知道辦理契約終止後就無法再恢復成原本的保單內容呢</td>\n",
       "      <td>好的了解與您確認好的了解與您確認本行人員並未鼓勵或勸誘已契約終止的方式中止的方式來購買新保單...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.466667</td>\n",
       "      <td>請問分行的是否有向您說明產品內容並確認符合您的需求</td>\n",
       "      <td>好的謝謝您接下來幾項問題向您確認我們有收到您安聯人壽保單號碼ql的契約終止文件一份請問您是否...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8955</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>請問您本次辦理貸款及保險是否有新申請玉山網路銀行</td>\n",
       "      <td>謝謝您那除了辦理貸款或保單借款需支付的本金及利息外還有該產品可能發生之相關風險及最大可能損失...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8956</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>請問您本次辦理貸款及保險是否有新申請玉山網路銀行</td>\n",
       "      <td>謝謝您那本行人員並未鼓勵或勸誘辦理貸款保單借款保單解約保單終止及定存解約方式購買保險請問是否...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8960</th>\n",
       "      <td>0.645161</td>\n",
       "      <td>感謝您近期透過本行投保人壽繳費年期為年躉繳依照保險法令的要求為保障您的權益稍後電話訪問內容將...</td>\n",
       "      <td>依照保險法令的要求為保障您的權益稍後電話訪問的內容會全程錄音請問您同意嗎</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8961</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>請問您是否知道辦理契約終止後就無法再恢復成原本的保單內容呢</td>\n",
       "      <td>好的謝謝您以下幾個問題與您確認我們有收到您安聯人壽保單號碼ql一一九七三九八三的契約終止文件...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8963</th>\n",
       "      <td>0.388889</td>\n",
       "      <td>請問您是否知道辦理契約終止後就無法再恢復成原本的保單內容呢</td>\n",
       "      <td>好的方便請問您這次辦理契約終止的資金用途為何呢</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2235 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ratio                                               simi  \\\n",
       "15    0.666667                                請問法定代理人欄位是否由您本人親自簽名   \n",
       "20    0.444444                      請問您是否知道辦理契約終止後就無法再恢復成原本的保單內容呢   \n",
       "22    0.333333                      請問您是否知道辦理契約終止後就無法再恢復成原本的保單內容呢   \n",
       "23    0.388889                      請問您是否知道辦理契約終止後就無法再恢復成原本的保單內容呢   \n",
       "28    0.466667                          請問分行的是否有向您說明產品內容並確認符合您的需求   \n",
       "...        ...                                                ...   \n",
       "8955  0.500000                           請問您本次辦理貸款及保險是否有新申請玉山網路銀行   \n",
       "8956  0.571429                           請問您本次辦理貸款及保險是否有新申請玉山網路銀行   \n",
       "8960  0.645161  感謝您近期透過本行投保人壽繳費年期為年躉繳依照保險法令的要求為保障您的權益稍後電話訪問內容將...   \n",
       "8961  0.444444                      請問您是否知道辦理契約終止後就無法再恢復成原本的保單內容呢   \n",
       "8963  0.388889                      請問您是否知道辦理契約終止後就無法再恢復成原本的保單內容呢   \n",
       "\n",
       "                                                   text  \n",
       "15    好的請問您這次辦理契約終止以及部分提領的文件是否藉由林森分行的林冠廷在旁邊協助並由要保人您親...  \n",
       "20    好的謝謝您接下來幾項問題項您確認我們有收到您安聯人壽保單號碼ql的契約終止文件一份請問您是否...  \n",
       "22                             好的好的方便問您這次辦理契約終止的資金用途為何呢  \n",
       "23    好的了解與您確認好的了解與您確認本行人員並未鼓勵或勸誘已契約終止的方式中止的方式來購買新保單...  \n",
       "28    好的謝謝您接下來幾項問題向您確認我們有收到您安聯人壽保單號碼ql的契約終止文件一份請問您是否...  \n",
       "...                                                 ...  \n",
       "8955  謝謝您那除了辦理貸款或保單借款需支付的本金及利息外還有該產品可能發生之相關風險及最大可能損失...  \n",
       "8956  謝謝您那本行人員並未鼓勵或勸誘辦理貸款保單借款保單解約保單終止及定存解約方式購買保險請問是否...  \n",
       "8960               依照保險法令的要求為保障您的權益稍後電話訪問的內容會全程錄音請問您同意嗎  \n",
       "8961  好的謝謝您以下幾個問題與您確認我們有收到您安聯人壽保單號碼ql一一九七三九八三的契約終止文件...  \n",
       "8963                            好的方便請問您這次辦理契約終止的資金用途為何呢  \n",
       "\n",
       "[2235 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['ratio']<0.7, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/jovyan/.local/lib/python3.6/site-packages/esun_phoneme_tool/jieba_dict_pool/general_jieba_0002.txt ...\n",
      "2021-04-28 11:50:11,249 DEBUG: Building prefix dict from /home/jovyan/.local/lib/python3.6/site-packages/esun_phoneme_tool/jieba_dict_pool/general_jieba_0002.txt ...\n",
      "Loading model from cache /tmp/jieba.u9c666b2c1c48c114768336a4722d3b92.cache\n",
      "2021-04-28 11:50:11,252 DEBUG: Loading model from cache /tmp/jieba.u9c666b2c1c48c114768336a4722d3b92.cache\n",
      "Loading model cost 0.570 seconds.\n",
      "2021-04-28 11:50:11,821 DEBUG: Loading model cost 0.570 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "2021-04-28 11:50:11,822 DEBUG: Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 實驗4 - stopwords 清理\n",
    "tokenizer = Tokenizer()\n",
    "stop_words = '玉山|玉山銀行|保代|保代部|保險代理部|您好|你好|這裡|這邊|之後|打擾|敝姓|員工|編號|員編|總行|消金|中心|不好意思|呃|嗯|感謝|謝謝'\n",
    "\n",
    "# clean stop words\n",
    "token_texts = []\n",
    "for text in texts:\n",
    "    tokens, token_types = tokenizer.tokenize(text[0])\n",
    "    tmp_tokens = []\n",
    "    for token, token_type in zip(tokens, token_types):\n",
    "        if token_type == 'CN' and token not in stop_words.split('|'):\n",
    "            tmp_tokens.append(token)\n",
    "    token_texts.append([''.join(tmp_tokens), text[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7172"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(8966*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[您好這裡是玉山銀行您好這裡是玉山銀行保險代理部敝姓唐員工編號請問是湯拱運儲小姐本人嗎您好,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[呃您好這邊是玉山銀行總行保險代理部敝姓唐員工編號請問是江淑華小姐本人嗎呃呃您好請問她在嗎方...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[您好呃您好這邊是玉山銀行總行保險代理部敝姓唐員工編號請問是江淑華小姐本人嗎, 嘿您好對]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[呃您好不好意思打擾您感謝您近期透過本行辦理首期保費繳交憑證那依照保險法令的要求為保證您的權...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[您好這裡是玉山銀行總行保代部敝姓張員工編號請問是林建銘先生本人嗎, 喂嘿是]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8961</th>\n",
       "      <td>[好的謝謝您以下幾個問題與您確認我們有收到您安聯人壽保單號碼ql一一九七三九八三的契約終止文...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8962</th>\n",
       "      <td>[好的請問您這次辦理契約終止的文件是否皆由士林分行的林傳仲在旁邊協助並由要保人您本人親自簽名...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8963</th>\n",
       "      <td>[好的方便請問您這次辦理契約終止的資金用途為何呢, 呃家用]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8964</th>\n",
       "      <td>[好的了解請問您是否知道辦理契約終止後就無法再恢復成原本的保單內容呢, 嗯哼可以知道嗯]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8965</th>\n",
       "      <td>[您好請問是李蘭君小姐本人嗎, 喂你好對對]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8966 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     [您好這裡是玉山銀行您好這裡是玉山銀行保險代理部敝姓唐員工編號請問是湯拱運儲小姐本人嗎您好,...      0\n",
       "1     [呃您好這邊是玉山銀行總行保險代理部敝姓唐員工編號請問是江淑華小姐本人嗎呃呃您好請問她在嗎方...      0\n",
       "2         [您好呃您好這邊是玉山銀行總行保險代理部敝姓唐員工編號請問是江淑華小姐本人嗎, 嘿您好對]      1\n",
       "3     [呃您好不好意思打擾您感謝您近期透過本行辦理首期保費繳交憑證那依照保險法令的要求為保證您的權...      1\n",
       "4               [您好這裡是玉山銀行總行保代部敝姓張員工編號請問是林建銘先生本人嗎, 喂嘿是]      1\n",
       "...                                                 ...    ...\n",
       "8961  [好的謝謝您以下幾個問題與您確認我們有收到您安聯人壽保單號碼ql一一九七三九八三的契約終止文...      1\n",
       "8962  [好的請問您這次辦理契約終止的文件是否皆由士林分行的林傳仲在旁邊協助並由要保人您本人親自簽名...      1\n",
       "8963                     [好的方便請問您這次辦理契約終止的資金用途為何呢, 呃家用]      1\n",
       "8964       [好的了解請問您是否知道辦理契約終止後就無法再恢復成原本的保單內容呢, 嗯哼可以知道嗯]      1\n",
       "8965                             [您好請問是李蘭君小姐本人嗎, 喂你好對對]      1\n",
       "\n",
       "[8966 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'text':texts, 'label':trues})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 09:48:10,988 INFO: device: cuda\n",
      "Some weights of the model checkpoint at /home/jovyan/if-beautiful-text/owen_dev/if_beautiful_text/cache_dir/bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/jovyan/if-beautiful-text/owen_dev/if_beautiful_text/cache_dir/bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2021-04-29 09:48:25,520 INFO: train_loss [100]: 0.29440208233892917\n",
      "evaluate: 100%|██████████| 29/29 [00:07<00:00,  3.68it/s]\n",
      "2021-04-29 09:48:33,407 INFO: valid_evaluation: loss=0.2900136421466696, accuracy=0.8913043478260869, precision=1.0, recall=0.034653465346534656, f1=0.06698564593301436\n",
      "2021-04-29 09:48:43,658 INFO: train_loss [200]: 0.1901419111341238\n",
      "evaluate: 100%|██████████| 29/29 [00:08<00:00,  3.54it/s]\n",
      "2021-04-29 09:48:51,851 INFO: valid_evaluation: loss=0.20392931496788716, accuracy=0.9202898550724637, precision=0.9682539682539683, recall=0.30198019801980197, f1=0.46037735849056605\n",
      "2021-04-29 09:49:01,757 INFO: train_loss [300]: 0.140938870459795\n",
      "evaluate: 100%|██████████| 29/29 [00:07<00:00,  3.67it/s]\n",
      "2021-04-29 09:49:09,677 INFO: valid_evaluation: loss=0.1687844962138554, accuracy=0.9470457079152731, precision=0.8203592814371258, recall=0.6782178217821783, f1=0.7425474254742548\n",
      "2021-04-29 09:49:20,429 INFO: train_loss [400]: 0.09829029157757758\n",
      "evaluate: 100%|██████████| 29/29 [00:08<00:00,  3.58it/s]\n",
      "2021-04-29 09:49:28,528 INFO: valid_evaluation: loss=0.13510968073688703, accuracy=0.9459308807134894, precision=0.7253218884120172, recall=0.8366336633663366, f1=0.7770114942528735\n",
      "2021-04-29 09:49:39,361 INFO: train_loss [500]: 0.15542736686766148\n",
      "evaluate: 100%|██████████| 29/29 [00:08<00:00,  3.61it/s]\n",
      "2021-04-29 09:49:47,397 INFO: valid_evaluation: loss=0.11589445443502788, accuracy=0.9615384615384616, precision=0.8375634517766497, recall=0.8168316831683168, f1=0.8270676691729323\n",
      "2021-04-29 09:49:57,793 INFO: train_loss [600]: 0.12203853990882635\n",
      "evaluate: 100%|██████████| 29/29 [00:08<00:00,  3.59it/s]\n",
      "2021-04-29 09:50:05,877 INFO: valid_evaluation: loss=0.1448686826845695, accuracy=0.9492753623188406, precision=0.9512195121951219, recall=0.5792079207920792, f1=0.72\n",
      "2021-04-29 09:50:15,968 INFO: train_loss [700]: 0.08773725792765617\n",
      "evaluate: 100%|██████████| 29/29 [00:07<00:00,  3.64it/s]\n",
      "2021-04-29 09:50:23,936 INFO: valid_evaluation: loss=0.10592393101803188, accuracy=0.9665551839464883, precision=0.8736842105263158, recall=0.8217821782178217, f1=0.846938775510204\n",
      "2021-04-29 09:50:34,317 INFO: train_loss [800]: 0.11092828899621963\n",
      "evaluate: 100%|██████████| 29/29 [00:08<00:00,  3.59it/s]\n",
      "2021-04-29 09:50:42,406 INFO: valid_evaluation: loss=0.11654143369403379, accuracy=0.9615384615384616, precision=0.9079754601226994, recall=0.7326732673267327, f1=0.810958904109589\n",
      "2021-04-29 09:50:54,294 INFO: train_loss [900]: 0.09645295117050409\n",
      "evaluate: 100%|██████████| 29/29 [00:07<00:00,  3.71it/s]\n",
      "2021-04-29 09:51:02,122 INFO: valid_evaluation: loss=0.12377750520305388, accuracy=0.9637681159420289, precision=0.9254658385093167, recall=0.7376237623762376, f1=0.8209366391184572\n",
      "2021-04-29 09:51:12,371 INFO: train_loss [1000]: 0.09410144794732332\n",
      "evaluate: 100%|██████████| 29/29 [00:07<00:00,  3.64it/s]\n",
      "2021-04-29 09:51:20,348 INFO: valid_evaluation: loss=0.1325847893824865, accuracy=0.9609810479375697, precision=0.94, recall=0.698019801980198, f1=0.8011363636363636\n",
      "2021-04-29 09:51:30,532 INFO: train_loss [1100]: 0.06610439527779817\n",
      "evaluate: 100%|██████████| 29/29 [00:07<00:00,  3.65it/s]\n",
      "2021-04-29 09:51:38,487 INFO: valid_evaluation: loss=0.1373003234380278, accuracy=0.9626532887402452, precision=0.9530201342281879, recall=0.7029702970297029, f1=0.809116809116809\n",
      "2021-04-29 09:51:49,190 INFO: train_loss [1200]: 0.06312614344060422\n",
      "evaluate: 100%|██████████| 29/29 [00:08<00:00,  3.42it/s]\n",
      "2021-04-29 09:51:57,684 INFO: valid_evaluation: loss=0.1158423347976701, accuracy=0.9632107023411371, precision=0.930379746835443, recall=0.7277227722772277, f1=0.8166666666666667\n",
      "2021-04-29 09:52:08,149 INFO: train_loss [1300]: 0.06168298121541738\n",
      "evaluate: 100%|██████████| 29/29 [00:07<00:00,  3.66it/s]\n",
      "2021-04-29 09:52:16,071 INFO: valid_evaluation: loss=0.11995366302415214, accuracy=0.9648829431438127, precision=0.9112426035502958, recall=0.7623762376237624, f1=0.830188679245283\n",
      "2021-04-29 09:52:26,542 INFO: train_loss [1400]: 0.05354082137346268\n",
      "evaluate: 100%|██████████| 29/29 [00:07<00:00,  3.71it/s]\n",
      "2021-04-29 09:52:34,355 INFO: valid_evaluation: loss=0.12229032987921402, accuracy=0.9632107023411371, precision=0.9, recall=0.7574257425742574, f1=0.8225806451612903\n",
      "2021-04-29 09:52:44,745 INFO: train_loss [1500]: 0.0637470668181777\n",
      "evaluate: 100%|██████████| 29/29 [00:07<00:00,  3.68it/s]\n",
      "2021-04-29 09:52:52,643 INFO: valid_evaluation: loss=0.13036481907655453, accuracy=0.9671125975473801, precision=0.9181286549707602, recall=0.7772277227722773, f1=0.8418230563002682\n",
      "2021-04-29 09:53:03,306 INFO: train_loss [1600]: 0.06421466685831546\n",
      "evaluate: 100%|██████████| 29/29 [00:08<00:00,  3.61it/s]\n",
      "2021-04-29 09:53:11,332 INFO: valid_evaluation: loss=0.11759687728922942, accuracy=0.9659977703455964, precision=0.9433962264150944, recall=0.7425742574257426, f1=0.8310249307479224\n",
      "2021-04-29 09:53:21,340 INFO: train_loss [1700]: 0.05561396490782499\n",
      "evaluate: 100%|██████████| 29/29 [00:08<00:00,  3.58it/s]\n",
      "2021-04-29 09:53:29,449 INFO: valid_evaluation: loss=0.1421631776952538, accuracy=0.9620958751393534, precision=0.9527027027027027, recall=0.698019801980198, f1=0.8057142857142856\n",
      "2021-04-29 09:53:40,798 INFO: train_loss [1800]: 0.0632329211384058\n",
      "evaluate: 100%|██████████| 29/29 [00:08<00:00,  3.53it/s]\n",
      "2021-04-29 09:53:49,025 INFO: valid_evaluation: loss=0.12166195627751536, accuracy=0.9643255295429208, precision=0.9107142857142857, recall=0.7574257425742574, f1=0.8270270270270269\n",
      "2021-04-29 09:53:59,792 INFO: train_loss [1900]: 0.022899933010339737\n",
      "evaluate: 100%|██████████| 29/29 [00:08<00:00,  3.62it/s]\n",
      "2021-04-29 09:54:07,804 INFO: valid_evaluation: loss=0.12722286431054616, accuracy=0.9626532887402452, precision=0.9090909090909091, recall=0.7425742574257426, f1=0.8174386920980926\n",
      "2021-04-29 09:54:18,290 INFO: train_loss [2000]: 0.027183385640382765\n",
      "evaluate: 100%|██████████| 29/29 [00:07<00:00,  3.68it/s]\n",
      "2021-04-29 09:54:26,179 INFO: valid_evaluation: loss=0.13586802552614746, accuracy=0.9648829431438127, precision=0.9371069182389937, recall=0.7376237623762376, f1=0.8254847645429362\n",
      "2021-04-29 09:54:36,911 INFO: train_loss [2100]: 0.03542233105748892\n",
      "evaluate: 100%|██████████| 29/29 [00:08<00:00,  3.61it/s]\n",
      "2021-04-29 09:54:44,951 INFO: valid_evaluation: loss=0.13486373485547715, accuracy=0.9648829431438127, precision=0.9371069182389937, recall=0.7376237623762376, f1=0.8254847645429362\n",
      "2021-04-29 09:54:55,035 INFO: train_loss [2200]: 0.06464972268790006\n",
      "evaluate: 100%|██████████| 29/29 [00:08<00:00,  3.49it/s]\n",
      "2021-04-29 09:55:03,354 INFO: valid_evaluation: loss=0.12977308773531995, accuracy=0.9654403567447045, precision=0.926829268292683, recall=0.7524752475247525, f1=0.8306010928961749\n",
      "2021-04-29 09:55:13,255 INFO: train_loss [2300]: 0.02160355681553483\n",
      "evaluate: 100%|██████████| 29/29 [00:08<00:00,  3.61it/s]\n",
      "2021-04-29 09:55:21,286 INFO: valid_evaluation: loss=0.13501226664360227, accuracy=0.9643255295429208, precision=0.9423076923076923, recall=0.7277227722772277, f1=0.8212290502793296\n",
      "2021-04-29 09:55:31,128 INFO: train_loss [2400]: 0.039600186981260777\n",
      "evaluate: 100%|██████████| 29/29 [00:07<00:00,  3.70it/s]\n",
      "2021-04-29 09:55:38,974 INFO: valid_evaluation: loss=0.12867516545771523, accuracy=0.9671125975473801, precision=0.9386503067484663, recall=0.7574257425742574, f1=0.8383561643835616\n",
      "2021-04-29 09:55:49,285 INFO: train_loss [2500]: 0.022367812618613245\n",
      "evaluate: 100%|██████████| 29/29 [00:07<00:00,  3.76it/s]\n",
      "2021-04-29 09:55:57,010 INFO: valid_evaluation: loss=0.13629246287947072, accuracy=0.9654403567447045, precision=0.9430379746835443, recall=0.7376237623762376, f1=0.8277777777777777\n",
      "2021-04-29 09:56:07,546 INFO: train_loss [2600]: 0.04244439709931612\n",
      "evaluate: 100%|██████████| 29/29 [00:08<00:00,  3.61it/s]\n",
      "2021-04-29 09:56:15,590 INFO: valid_evaluation: loss=0.1380416681861569, accuracy=0.9659977703455964, precision=0.937888198757764, recall=0.7475247524752475, f1=0.8319559228650137\n",
      "2021-04-29 09:56:26,232 INFO: train_loss [2700]: 0.02751549143344164\n",
      "evaluate: 100%|██████████| 29/29 [00:07<00:00,  3.71it/s]\n",
      "2021-04-29 09:56:34,047 INFO: valid_evaluation: loss=0.13001121776499625, accuracy=0.9659977703455964, precision=0.937888198757764, recall=0.7475247524752475, f1=0.8319559228650137\n",
      "2021-04-29 09:56:44,065 INFO: train_loss [2800]: 0.017334499545395374\n",
      "evaluate: 100%|██████████| 29/29 [00:07<00:00,  3.71it/s]\n",
      "2021-04-29 09:56:51,894 INFO: valid_evaluation: loss=0.13397992177513138, accuracy=0.9671125975473801, precision=0.9386503067484663, recall=0.7574257425742574, f1=0.8383561643835616\n",
      "2021-04-29 09:57:02,138 INFO: train_loss [2900]: 0.027407159209251405\n",
      "evaluate: 100%|██████████| 29/29 [00:08<00:00,  3.47it/s]\n",
      "2021-04-29 09:57:10,491 INFO: valid_evaluation: loss=0.14047756919573093, accuracy=0.9665551839464883, precision=0.94375, recall=0.7475247524752475, f1=0.8342541436464087\n",
      "2021-04-29 09:57:20,924 INFO: train_loss [3000]: 0.020792632699012756\n",
      "evaluate: 100%|██████████| 29/29 [00:07<00:00,  3.63it/s]\n",
      "2021-04-29 09:57:28,915 INFO: valid_evaluation: loss=0.13424820435265528, accuracy=0.9682274247491639, precision=0.9341317365269461, recall=0.7722772277227723, f1=0.8455284552845528\n",
      "2021-04-29 09:57:39,355 INFO: train_loss [3100]: 0.028628385588526727\n",
      "evaluate: 100%|██████████| 29/29 [00:08<00:00,  3.62it/s]\n",
      "2021-04-29 09:57:47,370 INFO: valid_evaluation: loss=0.1376272032743898, accuracy=0.967670011148272, precision=0.9390243902439024, recall=0.7623762376237624, f1=0.8415300546448088\n",
      "2021-04-29 09:57:57,502 INFO: train_loss [3200]: 0.03513286642730236\n",
      "evaluate: 100%|██████████| 29/29 [00:07<00:00,  3.63it/s]\n",
      "2021-04-29 09:58:05,506 INFO: valid_evaluation: loss=0.12987761321509705, accuracy=0.967670011148272, precision=0.9390243902439024, recall=0.7623762376237624, f1=0.8415300546448088\n",
      "2021-04-29 09:58:16,709 INFO: train_loss [3300]: 0.02670317642390728\n",
      "evaluate: 100%|██████████| 29/29 [00:07<00:00,  3.64it/s]\n",
      "2021-04-29 09:58:24,685 INFO: valid_evaluation: loss=0.13339919368897019, accuracy=0.9671125975473801, precision=0.9386503067484663, recall=0.7574257425742574, f1=0.8383561643835616\n",
      "2021-04-29 09:58:35,036 INFO: train_loss [3400]: 0.027212941274046897\n",
      "evaluate: 100%|██████████| 29/29 [00:07<00:00,  3.65it/s]\n",
      "2021-04-29 09:58:42,981 INFO: valid_evaluation: loss=0.1390312403793736, accuracy=0.9654403567447045, precision=0.9375, recall=0.7425742574257426, f1=0.8287292817679559\n",
      "2021-04-29 09:58:53,197 INFO: train_loss [3500]: 0.033655883111059666\n",
      "evaluate: 100%|██████████| 29/29 [00:08<00:00,  3.58it/s]\n",
      "2021-04-29 09:59:01,295 INFO: valid_evaluation: loss=0.13664071900962754, accuracy=0.967670011148272, precision=0.9444444444444444, recall=0.7574257425742574, f1=0.8406593406593407\n",
      "2021-04-29 09:59:12,530 INFO: train_loss [3600]: 0.03731629941612482\n",
      "evaluate: 100%|██████████| 29/29 [00:07<00:00,  3.64it/s]\n",
      "2021-04-29 09:59:20,506 INFO: valid_evaluation: loss=0.1332154058819187, accuracy=0.9659977703455964, precision=0.937888198757764, recall=0.7475247524752475, f1=0.8319559228650137\n",
      "2021-04-29 09:59:31,244 INFO: train_loss [3700]: 0.0200675742700696\n",
      "evaluate: 100%|██████████| 29/29 [00:07<00:00,  3.67it/s]\n",
      "2021-04-29 09:59:39,144 INFO: valid_evaluation: loss=0.13367950909867368, accuracy=0.9659977703455964, precision=0.937888198757764, recall=0.7475247524752475, f1=0.8319559228650137\n",
      "2021-04-29 09:59:49,581 INFO: train_loss [3800]: 0.011065753474831581\n",
      "evaluate: 100%|██████████| 29/29 [00:07<00:00,  3.72it/s]\n",
      "2021-04-29 09:59:57,384 INFO: valid_evaluation: loss=0.13407795815246887, accuracy=0.9665551839464883, precision=0.9382716049382716, recall=0.7524752475247525, f1=0.8351648351648351\n",
      "2021-04-29 10:00:07,363 INFO: train_loss [3900]: 0.012767303325235844\n",
      "evaluate: 100%|██████████| 29/29 [00:08<00:00,  3.56it/s]\n",
      "2021-04-29 10:00:15,511 INFO: valid_evaluation: loss=0.13447530092350368, accuracy=0.9665551839464883, precision=0.9382716049382716, recall=0.7524752475247525, f1=0.8351648351648351\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-ba3ab7b8017c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# 原版\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 2021-04-28 12:06:25,462 INFO: valid_evaluation: loss=0.08820323068006285, accuracy=0.9682274247491639, precision=0.8877005347593583, recall=0.8217821782178217, f1=0.853470437017995\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 2021-04-28 13:13:16,066 INFO: valid_evaluation: loss=0.13116201947860676, accuracy=0.9671125975473801, precision=0.949685534591195, recall=0.7475247524752475, f1=0.8365650969529086\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-417d9344dab6>\u001b[0m in \u001b[0;36mbert_main\u001b[0;34m(texts, trues, model_dir, save_dir)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mis_running\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mloss_averager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-ec7a48ab5ac8>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(model, data, optimizer, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bert_main(tmp_texts, trues)\n",
    "# 原版\n",
    "# 2021-04-28 12:06:25,462 INFO: valid_evaluation: loss=0.08820323068006285, accuracy=0.9682274247491639, precision=0.8877005347593583, recall=0.8217821782178217, f1=0.853470437017995\n",
    "# 2021-04-28 13:13:16,066 INFO: valid_evaluation: loss=0.13116201947860676, accuracy=0.9671125975473801, precision=0.949685534591195, recall=0.7475247524752475, f1=0.8365650969529086\n",
    "\n",
    "# 權威控制\n",
    "# 2021-04-28 13:50:51,148 INFO: valid_evaluation: loss=0.13724370429228092, accuracy=0.9671125975473801, precision=0.9333333333333333, recall=0.7623762376237624, f1=0.8392370572207085\n",
    "# 2021-04-28 13:59:52,614 INFO: valid_evaluation: loss=0.13895737874353753, accuracy=0.9665551839464883, precision=0.9329268292682927, recall=0.7574257425742574, f1=0.8360655737704918\n",
    "\n",
    "# 僅自定義關鍵字\n",
    "# 2021-04-28 13:18:45,353 INFO: valid_evaluation: loss=0.11003045374848719, accuracy=0.9637681159420289, precision=0.8442211055276382, recall=0.8316831683168316, f1=0.8379052369077307\n",
    "# 2021-04-28 13:30:49,645 INFO: valid_evaluation: loss=0.13326888428679828, accuracy=0.9626532887402452, precision=0.9041916167664671, recall=0.7475247524752475, f1=0.8184281842818428\n",
    "\n",
    "# 去除自定義停用字\n",
    "# 2021-04-28 11:55:27,290 INFO: valid_evaluation: loss=0.09807703650460162, accuracy=0.967670011148272, precision=0.9044943820224719, recall=0.7970297029702971, f1=0.8473684210526315\n",
    "# 2021-04-28 12:00:14,627 INFO: valid_evaluation: loss=0.14567224899756498, accuracy=0.9654403567447045, precision=0.9605263157894737, recall=0.7227722772277227, f1=0.824858757062147\n",
    "\n",
    "# tf-idf top 5 + 原句\n",
    "# 2021-04-28 16:14:43,515 INFO: valid_evaluation: loss=0.1148602656120884, accuracy=0.9654403567447045, precision=0.8932584269662921, recall=0.7871287128712872, f1=0.8368421052631579\n",
    "# 2021-04-28 16:26:33,965 INFO: valid_evaluation: loss=0.1434477025578762, accuracy=0.9648829431438127, precision=0.9112426035502958, recall=0.7623762376237624, f1=0.830188679245283\n",
    "\n",
    "# set版權威控制\n",
    "# 2021-04-29 09:50:23,936 INFO: valid_evaluation: loss=0.10592393101803188, accuracy=0.9665551839464883, precision=0.8736842105263158, recall=0.8217821782178217, f1=0.846938775510204\n",
    "# 2021-04-29 09:56:15,590 INFO: valid_evaluation: loss=0.1380416681861569, accuracy=0.9659977703455964, precision=0.937888198757764, recall=0.7475247524752475, f1=0.8319559228650137\n",
    "\n",
    "# rule-base\n",
    "# 'precision': 0.7540106951871658,  'recall': 0.6588785046728972, 'f1': 0.7032418952618454"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.iloc[:7000, :].copy()\n",
    "df_valid = df.iloc[7000:8000, :].copy()\n",
    "df_test = df.iloc[8000:, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule-base 判斷\n",
    "import re\n",
    "\n",
    "preds = []\n",
    "for text in texts[int(0.8*8966):]:   \n",
    "    wrong_ans_1 = '什麼'\n",
    "    correct_ans_1 = '理解|了解|知道|明白|是|ok|可以|沒錯|對|好|有|同意|正確|清楚|曉得|懂|不影響|不會|沒問題|確認'\n",
    "    ques_1 = '不影響|不會|不會影響'\n",
    "    ques_1_ans = '不影響|不會|不會影響'\n",
    "    ques_2 = '日常|支出|用途'\n",
    "    ques_2_ans = '儲蓄|貸款|投資|家用|基金|買|房屋|車|購物|生活|支出'\n",
    "    ques_3 = '資金來源'\n",
    "    ques_3_ans = '薪水|薪資|儲蓄'\n",
    "    \n",
    "    rst = 0\n",
    "    # 問題對照規則\n",
    "    if any([re.search(a, text[0]) for a in ques_1.split('|')]):\n",
    "        if any([re.search(a, text[1]) for a in ques_1_ans.split('|')]):\n",
    "            rst = 1\n",
    "    if any([re.search(a, text[0]) for a in ques_2.split('|')]):\n",
    "        if any([re.search(a, text[1]) for a in ques_2_ans.split('|')]):\n",
    "            rst = 1\n",
    "    if any([re.search(a, text[0]) for a in ques_3.split('|')]):\n",
    "        if any([re.search(a, text[1]) for a in ques_3_ans.split('|')]):\n",
    "            rst = 1\n",
    "            \n",
    "    # 一般規則\n",
    "    if any([re.search(a, text[1]) for a in wrong_ans_1.split('|')]):\n",
    "        rst = 0\n",
    "    elif any([re.search('不'+a, text[1]) for a in correct_ans_1.split('|')]):\n",
    "        rst = 0\n",
    "    elif any([re.search(a, text[1]) for a in correct_ans_1.split('|')]):\n",
    "        rst = 1\n",
    "        \n",
    "    preds.append(rst)\n",
    "labels = trues[int(0.8*8966):]\n",
    "# labels = df.label.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule-base 判斷\n",
    "import re\n",
    "\n",
    "preds = []\n",
    "for text in texts[int(0.8*8966):]:   \n",
    "    wrong_ans_1 = '什麼'\n",
    "    correct_ans_1 = '理解|了解|知道|明白|是|ok|可以|沒錯|對|好|有|同意|正確|清楚|曉得|懂|不影響|不會|沒問題|確認'\n",
    "    ques_1 = '不影響|不會|不會影響'\n",
    "    ques_1_ans = '不影響|不會|不會影響'\n",
    "    ques_2 = '日常|支出|用途'\n",
    "    ques_2_ans = '儲蓄|貸款|投資|家用|基金|買|房屋|車|購物|生活|支出'\n",
    "    ques_3 = '資金來源'\n",
    "    ques_3_ans = '薪水|薪資|儲蓄'\n",
    "    \n",
    "    rst = 0\n",
    "    # 問題對照規則\n",
    "#     if any([re.search(a, text[0]) for a in ques_1.split('|')]):\n",
    "#         if any([re.search(a, text[1]) for a in ques_1_ans.split('|')]):\n",
    "#             rst = 1\n",
    "#     if any([re.search(a, text[0]) for a in ques_2.split('|')]):\n",
    "#         if any([re.search(a, text[1]) for a in ques_2_ans.split('|')]):\n",
    "#             rst = 1\n",
    "#     if any([re.search(a, text[0]) for a in ques_3.split('|')]):\n",
    "#         if any([re.search(a, text[1]) for a in ques_3_ans.split('|')]):\n",
    "#             rst = 1\n",
    "            \n",
    "    # 一般規則\n",
    "    if any([re.search(a, text[1]) for a in wrong_ans_1.split('|')]):\n",
    "        rst = 0\n",
    "#     elif any([re.search('不'+a, text[1]) for a in correct_ans_1.split('|')]):\n",
    "#         rst = 0\n",
    "    elif any([re.search(a, text[1]) for a in correct_ans_1.split('|')]):\n",
    "        rst = 1\n",
    "        \n",
    "    preds.append(rst)\n",
    "labels = trues[int(0.8*8966):]\n",
    "# labels = df.label.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "誤判為 對 ['好的請問您本次投保繳交保費的資金來源是否為保單解約金呢', '噢是噢']\n",
      "誤判為 對 ['呃您好不好意思打擾您感謝您近期透過本行辦理契約終止依照保險法令的要求為保障您的權益稍後電話訪問的內容會全程錄音請問您同意嗎', '欸我聽不太清楚捏']\n",
      "誤判為 對 ['謝謝您的同意那為了維護您的資料安全跟您簡單核對一下您的身分證字號是t二零一四三六請問後三碼是多少呢', '啊不是你你講這樣我不太清楚欸']\n",
      "誤判為 對 ['好的請問您此次辦理契約終止的文件是否皆由斗六分行的顏慧凌在旁邊協助並由要保人您本人親自簽名呢', '噢你是說斗六玉山銀行']\n",
      "誤判為 對 ['對就是是不是由斗六分行的顏慧凌在旁邊協助您那並且是由要保人就是您本人親自在文件上簽名呢', '好']\n",
      "誤判為 對 ['好的那方便請問您此次辦理契約終止的資金用途為何呢', '呃對我要HTR那個第七年了嗎我要第七年了HTR嗯']\n",
      "誤判為 對 ['您好呃請問這部分有清楚嗎', '喂喂欸我聽到了那我要說話嗎我要說是這樣子嗎']\n",
      "誤判為 對 ['嗯好的那再次與您確認本保單之規劃您是否已確實了解投保目的保險需求並經綜合考量財務狀況以及付費能力且不影響您的日常支出呢', '欸等一下我在忙好不好喂那個十']\n",
      "誤判為 對 ['呃您好好再跟您確認一下本保單之規劃您是否已確實了解投保目的保險需求並經綜合考量財務狀況以及付費能力且不影響您的日常支出呢', '欸你你你再說一次繳我我聽不大懂欸']\n",
      "誤判為 對 ['呃那薪資的部分呢', '薪資也是呃就應該是應該應該是這樣講對啦就是薪資然後存在保險裡面對在那個那個儲蓄裡面嘛']\n",
      "誤判為 對 ['對這部分有清楚嗎', '欸你嗯沒有聽很清楚你再講一次']\n",
      "誤判為 對 ['請問本次保費的資金來源是否為買賣不動產', '嘿噢不是是呃本本來就有的欸那個本來的存款嘿']\n",
      "誤判為 對 ['噢噢好請問招攬人員是否有提供您躉繳與分期繳分期繳等不同繳費方式選擇請問他有提供給你嗎', '呃不好意思那個']\n",
      "誤判為 對 ['依照保險法令的要求為保障您的權益稍後電話訪問的內容會全程錄音請問您同意嗎', '欸對呀欸好像是昨天嗎']\n",
      "誤判為 對 ['噢好的謝謝您那請問您是否知道辦理契約終止後就無法再恢復成原本的保單內容呢', '是昨天嗎對呀']\n",
      "誤判為 對 ['好謝謝呃嘿是那第四題是請問招攬人員是否有提供您躉繳與分期繳等不同繳費方式選擇', '是就一次繳完啊']\n",
      "誤判為 對 ['理是但接下來請問理專是否有說明產品內容並提供保險條款且您了解投保目的符合您的保險需求及付款能力並不影響日常支出呢', '現在有啦ok嘿嘿']\n",
      "誤判為 對 ['是這次的保費的資金來源是否為股票或基金呢', '嘿這個噢這個一半是股票吧一半是我自己錢啊']\n",
      "誤判為 對 ['那請問一下招攬人員是否有提供您一次繳清與分期繳等不同的繳費方式讓您做選擇請問有提供你做選擇嗎', '他是說一次繳清啊可以分期繳嗎']\n",
      "誤判為 對 ['是否有提供您一次繳清與分期繳等不同的繳費方式提你您做這樣的選擇', '他是說一次要繳清啊']\n",
      "誤判為 對 ['好的請問您本次投保繳交保費的資金來源是否為保單解約金呢', '聽不太懂意思']\n",
      "誤判為 對 ['好的那方便請問您此次辦理契約終止的資金用途為何呢', '有啊']\n",
      "誤判為 對 ['謝謝您噢那本次保費的資金來源是否為薪資以及股票或基金呢', '欸要選項是不是']\n",
      "誤判為 對 ['薪資以及股票或基金', '欸算是吧是嘿']\n",
      "誤判為 對 ['噢好的了解請問您是否知道辦理部分提領之後就無法恢復成原本的保單內容呢', '呃我我不知道聽不懂']\n",
      "誤判為 對 ['好謝謝那請問您是否知道本次購買是南山人壽的保險不是存款如果辦理解約將可能只領回部分已繳保費', '蛤你再念一次好快噢']\n",
      "誤判為 對 ['您好這裡是玉山銀行總行個金集中部請問是黃維寧小姐嗎您好', '喂你好']\n",
      "誤判為 對 ['好的那請問您此次辦理契約終止的資金用途為何呢呃是有其他的要用途要做使用嗎', '我司明白']\n",
      "誤判為 對 ['您好這裡是玉山銀行總行保代部敝姓姚員工編號二零二七一呃請問不好意思請問是呃廖徳坤小姐本人嗎', '喂是廖徳坤是先生等一下他是我弟']\n",
      "誤判為 對 ['是就是我們本次投保的資金來源保費的資金來源是否為儲蓄呢還是我們的來源', '我聽不懂這個']\n",
      "誤判為 對 ['嗯好的那請問您是否知道如果中途部分提領或是提前解約保險公司會依受理當時的保單帳戶價值扣除當年度的解約費用後再給付給您呢', '欸這件事情因為我倒是沒注意到中途解約']\n",
      "誤判為 對 ['好的好那請問您是否知道您所繳的保費會扣除相關費用後像是保單管理費或是保險成本等才進入投資帳戶呢', '對']\n",
      "誤判為 對 ['好的那請問您是否知道如果中途部分提領或是提前解約保險公司會依受理當時的保單帳戶價值扣除當年度的解約費用後再給付給您呢', '好像有吧']\n",
      "誤判為 對 ['呃這部分有清楚嗎嗯', '因為他跟我講說好像如果中途怎樣那不用自己擔那個費用啊']\n",
      "誤判為 對 ['保險公司會依受理當時的保單帳戶價值扣除當年度的解約費用後再給付給您呢', '這個我不是很清楚欸']\n",
      "誤判為 對 ['好的的那我這邊再問一次那個這一題這一題再問一下噢本次的保費資金來源是否為薪資以及股票或者是基金呢', '欸薪資跟基金都有']\n",
      "誤判為 對 ['對那與您確認本保單之規劃您是否已確實了解投保目的保險需求並經綜合考量財務狀況以及付費能力且不影響您的日常支出', '有']\n",
      "誤判為 對 ['呃是那您的資金用途是有家用還是其他用途嗎', '有啊']\n",
      "誤判為 對 ['或勸誘以契約終止的方式來購買新的保單請問是否正確呢', '嗯不一定再看是要保單還是基金對']\n",
      "誤判為 對 ['稍後電話訪問的內容會全程錄音請問您同意嗎', '嘿全程錄音可以嗎']\n",
      "誤判為 對 ['呃南山那邊那因為您有辦解約那我們要跟您做個簡單的電話訪問過程會錄音請問您同意嗎', '嗯嗯那個不要緊吧錄音起來有關係嗎']\n",
      "誤判為 對 ['呃對那這部分錄音的這部分您同意嗎', '看你們怎樣啦沒有妨礙到我我那個錢都有回到我們的戶口這樣就沒有問題了啦']\n",
      "誤判為 對 ['呃您說您先不解約嗎', 'HTR就解約了我都簽好了都寫好那個那個郵局的那個存簿的的號碼也都寫好了']\n",
      "誤判為 對 ['五七七二五九的契約終止文件一份請問您是否有申請呢', '嘿你說我怎樣我聽聽不太懂']\n",
      "誤判為 對 ['那請問您是否知道辦理保單解約之後就無法再恢復成原本的保單內容呢', '無法保啊不是那個玉山銀行也給我收多少錢都已經說好了']\n",
      "誤判為 對 ['嗯那啊那他有跟您說我們會有寄送交易確認簡訊的服務嗎就是寄到手機簡訊的這個服務', '我我沒有拿拿手機他說要連絡就是打這個電話給我啊']\n",
      "誤判為 對 ['好那請問招攬人員是否有提供您躉繳跟分期繳等不同繳費方式選擇', '我是分期繳還是躉繳']\n",
      "誤判為 對 ['依照保險法令的要求為保障您的權益稍後電話訪問的內容會全程錄音請問您同意嗎', '好可是我是HTR昨天已經打來了嗎']\n",
      "誤判為 對 ['好的那請問您本次投保繳交保費的資金來源是否為保單解約金呢', '嗯沒有就是']\n",
      "誤判為 對 ['呃就是您呃本次投保繳交保費的資金來源是否為保單解約金呢', '來源不完全是解約金如嘿']\n",
      "誤判為 對 ['呃就是我們有收到您保誠人壽的兩兩份保單解約的文件那請問您有申請嗎', '噢我有看過了有看過了']\n",
      "誤判為 對 ['呃所以您有申請這個兩份的保單解約是嗎', '這些這些是我兒子的事情啦我沒有管他啦']\n",
      "誤判為 對 ['好請問您此次投保繳交保費的資金來源是否為保單解約的費用', '知道']\n",
      "誤判為 對 ['您好不好意思打擾您感謝您近期透過本行辦理保單解約依照保險法令的要求為保障您的權益稍後電話訪問的內容會全程錄音請問您同意嗎', '你講慢一點好不好']\n",
      "誤判為 對 ['好的謝謝您那以下幾個問題跟您確認我們有收到您保誠人壽保單號碼七八四二一八五五以及七八八四三一五八的保單解約文件兩份請問您有申請嗎', '對沒有了']\n",
      "誤判為 對 ['噢好的謝謝您那提醒您保險公司收到您的文件後也有可能對您進行電話訪問那不好意思再跟您確認一下我們理專他在協助您辦理文件的時候有沒有跟您說我們會有寄送交易確認簡訊的這個服務呢', '這句話我聽不懂']\n",
      "誤判為 對 ['噢好的了解請問您是否知道辦理契約終止後就無法再恢復成原本的保單內容呢', '我知道那個保單只能保到大概七十幾歲吧然後再越年紀越大的話那個保費越貴所以根本沒辦法保']\n",
      "誤判為 對 ['請問您本次投保繳交保費的資金來源是否為儲蓄呢', '呃不是欸儲蓄保險啊是保險啊']\n",
      "誤判為 對 ['謝謝您噢那請問本好的謝謝您那請問本次保費的資金來源是否為儲蓄呢', '有嘿儲蓄']\n",
      "誤判為 對 ['是否有告知您會寄發交易確認簡訊那會寄送到您留存下來的手機號碼裡請問理專有告知您我們有這項服務嗎', '嗯應該有吧這個我不太記得了']\n",
      "誤判為 對 ['呃這部分有清楚嗎因為我需要比較肯定一點的回覆', '呃是沒有很清楚啦欸基本上我應該就是說不會去解欸中途解約']\n",
      "誤判為 對 ['嗯是那這部分您有清楚嗎', '呃還好啦']\n",
      "誤判為 對 ['依照保險法令的要求為保障您的權益稍後電話訪問的內容會全程錄音請問您同意嗎', '嘿他現在要訪問我啦他就就玉山啊HTR問說那個我我是不是有辦這個現在要同意不同意他說我是不是有辦這個HTR嘿啊']\n",
      "誤判為 對 ['和本次貸款並沒有搭售或不當行銷的情形發生請問是否正確', '對啊就是沒他另外貸款貸款的保險另外保險的啊']\n",
      "誤判為 對 ['呃呃我們有收到您安聯人壽保單號碼ql一零一零四五八八以及三商美邦人壽保單號碼八八一零零零七三零五八二還有保誠人壽保單號碼七八零四二四二五的呃契約終止文件總共有三分請問您是否有申請呢', '沒問題呀']\n",
      "誤判為 對 ['呃就是我們有收到您安聯人壽以及三商美邦人壽還有保誠人壽的契約終止文件有三分那請問您有申請嗎', '你聲音可以大聲一點嗎我聽不到欸']\n",
      "誤判為 對 ['呃那呃那是噢好我再重複一次噢', '投資儲蓄都有投資儲蓄']\n",
      "誤判為 對 ['謝謝您的同意那為了維護您的資料安全跟您簡單核對一下您的身分證字號是l一零零一六六請問後三碼是多少呢', '您說身分證字號是嗎']\n",
      "誤判為 對 ['張哲青先生本人嗎', '不是啊我彭瑞香啊']\n",
      "誤判為 對 ['呃您這次辦理那個保單解約的資金是有家用還是其他用途嗎', '我我有用啊有那個時候以以後有我再再減投保啊']\n",
      "誤判為 對 ['就無法再恢復成原本的保單內容', '沒有那個我沒有沒有HTR啦']\n",
      "誤判為 對 ['謝謝您噢那請問您這一次辦理契約終止的文件是否皆由竹南分行的杜育宏在旁邊協助並由要保人您本人親自簽名呢', '欸我我就我就全名欸我也沒有看就就簽就簽給他了那就HTR後退要退退HTR']\n",
      "誤判為 對 ['嗯好的那請問您本次投保繳交保費的資金來源是否為舊保單解約呢', '嗯嗯嗯對這個問題我可以再聽一次嗎']\n",
      "誤判為 對 ['好謝謝您好那再次與您確認本保單之規劃您是否已確實了解投保目的保險需求並經綜合考量財務狀況以及付費能力且不影響您的日常支出呢', '當然嗯嗯這一句可再再講一次好不好']\n",
      "誤判為 對 ['您好您好請問是蘇金川先生本人嗎', '電話號碼嘿啦是你哪裡']\n",
      "誤判為 對 ['好的那本次的保費資金來源啊是否為舊保單解約呢', '資金來源是否為舊保單解約']\n",
      "誤判為 對 ['謝謝您請問本次保費的資金來源是否為貸款及投資收回', '欸是那個我賣地的我土地賣掉的就是投資收回的']\n",
      "誤判為 對 ['請問本次保費的資金來源是否為貸款及投資回收', '那是投資回收']\n",
      "誤判為 對 ['好的那最後三十秒向您說明噢本行人員並未鼓勵或勸誘以辦理貸款保單借款定存解約或保單解約保單終止之方式從事投資理財或購買保險若有貸款保單借款定存解約或保單解約保單終止之資金運用於本次產品需事先審慎評估自身財務狀況與風險承受能力並願承擔因財務槓桿操作方式所面臨的風險及保單轉投保之權益損失除辦理貸款或保單借款需支付本金及利息外該產品可能發生之相關風險及最大可能損失請問您是否都了解呢', '嗯對嗯']\n",
      "誤判為 對 ['您好這裡是玉山銀行請問劉黃譚小姐在嗎', '喂聽不懂啦']\n",
      "誤判為 對 ['好那麼請問招攬人員是否有提供您躉繳也就是一次繳清與分期繳等不同繳費方式的選擇呢', '他他是跟我講說那個啊一次繳的啊躉繳的啊']\n",
      "誤判為 對 ['好請問您是否知道本次購買的是南山人壽的外幣保險不是存款需自行承擔匯率變動的風險呢', '這是保險噢']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.7361963190184049,\n",
       " 'recall': 0.594059405940594,\n",
       " 'f1': 0.6575342465753424}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp, fp, fn = 0, 0, 0    \n",
    "for i, (label, pred) in enumerate(zip(labels, preds)):\n",
    "    tp += 1 if (label, pred) == (0, 0) else 0\n",
    "    if (label, pred) == (1, 0):\n",
    "        fp += 1\n",
    "#         print('誤判為 錯', texts[7172+i])\n",
    "    else:\n",
    "        fp += 0\n",
    "    if (label, pred) == (0, 1):\n",
    "        fn += 1 \n",
    "        print('誤判為 對', texts[7172+i])\n",
    "    else:\n",
    "        fn += 0    \n",
    "    \n",
    "precision = tp / (tp + fp) if tp + fp > 0 else None\n",
    "recall = tp / (tp + fn) if tp + fn > 0 else None\n",
    "f1 = 2 / (1 / precision + 1 / recall) if precision and recall else None\n",
    "\n",
    "evaluation = {\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1': f1\n",
    "}\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'loc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-23cee23aca0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_rows'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'本人親自簽名'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question_x'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'loc'"
     ]
    }
   ],
   "source": [
    "# pd.set_option('display.max_rows', None)\n",
    "df = df.loc[df['question'].apply(lambda x: any(i in x for i in ['本人親自簽名'])), ['question', 'answer']]['question_x'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>label</th>\n",
       "      <th>question_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>您好這裡是玉山銀行您好這裡是玉山銀行保險代理部敝姓唐員工編號請問是湯拱運儲小姐本人嗎您好</td>\n",
       "      <td>喂喂喂</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>呃您好這邊是玉山銀行總行保險代理部敝姓唐員工編號請問是江淑華小姐本人嗎呃呃您好請問她在嗎方便...</td>\n",
       "      <td>喔謝謝</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>您好呃您好這邊是玉山銀行總行保險代理部敝姓唐員工編號請問是江淑華小姐本人嗎</td>\n",
       "      <td>嘿您好對</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>呃您好不好意思打擾您感謝您近期透過本行辦理首期保費繳交憑證那依照保險法令的要求為保證您的權益...</td>\n",
       "      <td>好可可以</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>您好這裡是玉山銀行總行保代部敝姓張員工編號請問是林建銘先生本人嗎</td>\n",
       "      <td>喂嘿是</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question answer  label question_x\n",
       "0       您好這裡是玉山銀行您好這裡是玉山銀行保險代理部敝姓唐員工編號請問是湯拱運儲小姐本人嗎您好    喂喂喂      0       None\n",
       "1  呃您好這邊是玉山銀行總行保險代理部敝姓唐員工編號請問是江淑華小姐本人嗎呃呃您好請問她在嗎方便...    喔謝謝      0       None\n",
       "2              您好呃您好這邊是玉山銀行總行保險代理部敝姓唐員工編號請問是江淑華小姐本人嗎   嘿您好對      1       None\n",
       "3  呃您好不好意思打擾您感謝您近期透過本行辦理首期保費繳交憑證那依照保險法令的要求為保證您的權益...   好可可以      1       None\n",
       "4                   您好這裡是玉山銀行總行保代部敝姓張員工編號請問是林建銘先生本人嗎    喂嘿是      1       None"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4523"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['question_x'].apply(lambda x: x != None))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
